# 1. ëª¨ë“  import ë¬¸
import pandas as pd
import numpy as np
import os
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
import cf
from mysql_loader import list_tables_in_database, load_data_from_mysql
from dense_finding import get_stock_items
from tqdm import tqdm
from telegram_utils import send_telegram_message
from datetime import datetime, timedelta
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error
from db_connection import DBConnectionManager
import pickle
import tensorflow as tf
from itertools import islice
from sklearn.model_selection import TimeSeriesSplit

# íŒŒì¼ ìƒë‹¨ì˜ import ì„¹ì…˜ì— ì¶”ê°€


# 2. ì „ì—­ ë³€ìˆ˜ ë° ìƒìˆ˜
COLUMNS_CHART_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']

COLUMNS_TRAINING_DATA = [
    'Open_to_LastClose', 'High_to_Close', 'Low_to_Close',
    'Close_to_LastClose', 'Volume_to_LastVolume',
    'Close_to_MA5', 'Volume_to_MA5',
    'Close_to_MA10', 'Volume_to_MA10',
    'Close_to_MA20', 'Volume_to_MA20',
    'Close_to_MA60', 'Volume_to_MA60',
    'Close_to_MA120', 'Volume_to_MA120',
    'Close_to_MA240', 'Volume_to_MA240',
]

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
model_dir = 'models'
os.makedirs(model_dir, exist_ok=True)

# 3. ëª¨ë“  í•¨ìˆ˜ ì •ì˜
def save_checkpoint(state, filename='checkpoint.pkl'):
    """í˜„ì¬ ì²˜ë¦¬ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    # ê¸°ì¡´ ë™ì¼ ìš©ë„ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    prefix = filename.split('.')[0].rsplit('_', 1)[0]
    for old_file in os.listdir('.'):
        if old_file.startswith(prefix) and old_file.endswith('.pkl') and old_file != filename:
            try:
                os.remove(old_file)
                print(f"Removed previous checkpoint: {old_file}")
            except:
                print(f"Could not remove old checkpoint: {old_file}")
    
    with open(filename, 'wb') as f:
        pickle.dump(state, f)
    print(f"Checkpoint saved to {filename}")

def load_checkpoint(filename='checkpoint.pkl'):
    """ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(filename):
        with open(filename, 'rb') as f:
            state = pickle.load(f)
        print(f"Checkpoint loaded from {filename}")
        return state
    return None

def save_training_checkpoint(state, filename='training_checkpoint.pkl'):
    """í˜„ì¬ í›ˆë ¨ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    prefix = filename.split('_checkpoint')[0] + '_checkpoint'
    for old_file in os.listdir('.'):
        if old_file.startswith(prefix) and old_file.endswith('.pkl') and old_file != filename:
            try:
                os.remove(old_file)
                print(f"Removed previous checkpoint: {old_file}")
            except:
                print(f"Could not remove old checkpoint: {old_file}")
    with open(filename, 'wb') as f:
        pickle.dump(state, f)
    print(f"Training checkpoint saved to {filename}")


def load_training_checkpoint(filename='training_checkpoint.pkl'):
    """ì €ì¥ëœ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(filename):
        with open(filename, 'rb') as f:
            state = pickle.load(f)
            print(f"Training checkpoint loaded from {filename}")
            return state
    return None
  
def clear_memory():
    import gc
    
    # ë³€ìˆ˜ ëª…ì‹œì  ì‚­ì œ
    local_vars = list(locals().items())
    for name, value in local_vars:
        if isinstance(value, (pd.DataFrame, np.ndarray, list)) and name not in ['df', 'result', 'return_value']:
            del locals()[name]
    
    # TensorFlow ë°±ì—”ë“œ ì„¸ì…˜ ì •ë¦¬
    tf.keras.backend.clear_session()
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    gc.collect()


def select_stocks_for_training(filtered_results):
    """ì‚¬ìš©ìê°€ í›ˆë ¨ì— ì‚¬ìš©í•  ì¢…ëª©ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜"""
    unique_codes = filtered_results['code_name'].unique()
    total_codes = len(unique_codes)
    
    print(f"\nì´ {total_codes}ê°œ ì¢…ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
    print("ì„ íƒ ë°©ë²•:")
    print("1. ì¢…ëª© ë²”ìœ„ë¡œ ì„ íƒ (ì˜ˆ: 1-50)")
    print("2. íŠ¹ì • ì¢…ëª© ì§ì ‘ ì…ë ¥ (ì˜ˆ: ì‚¼ì„±ì „ì,í˜„ëŒ€ì°¨,SKí•˜ì´ë‹‰ìŠ¤)")
    print("3. ëœë¤ ì„ íƒ (ì˜ˆ: random 30)")
    
    choice = input("ì„ íƒ ë°©ë²•ì„ ì…ë ¥í•˜ì„¸ìš” (1/2/3): ")
    
    if choice == '1':
        try:
            range_input = input("í›ˆë ¨ì— ì‚¬ìš©í•  ì¢…ëª© ë²”ìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1-50): ")
            start, end = map(int, range_input.split('-'))
            if start < 1:
                start = 1
            if end > total_codes:
                end = total_codes
            selected_codes = unique_codes[start-1:end]
            print(f"{len(selected_codes)}ê°œ ì¢…ëª© ì„ íƒë¨ ({start}ë²ˆë¶€í„° {end}ë²ˆê¹Œì§€)")
        except Exception as e:
            print(f"ë²”ìœ„ ì…ë ¥ ì˜¤ë¥˜: {e}. ëª¨ë“  ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return filtered_results
    
    elif choice == '2':
        try:
            specific_codes = input("í›ˆë ¨ì— ì‚¬ìš©í•  ì¢…ëª© ì½”ë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”: ").split(',')
            specific_codes = [code.strip() for code in specific_codes]
            selected_codes = [code for code in unique_codes if code in specific_codes]
            print(f"{len(selected_codes)}ê°œ ì¢…ëª© ì„ íƒë¨")
            if not selected_codes:
                print("ì¼ì¹˜í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return filtered_results
        except Exception as e:
            print(f"ì¢…ëª© ì…ë ¥ ì˜¤ë¥˜: {e}. ëª¨ë“  ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return filtered_results
    
    elif choice == '3':
        try:
            random_input = input("ëœë¤í•˜ê²Œ ì„ íƒí•  ì¢…ëª© ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: random 30): ")
            num_stocks = int(random_input.split()[1])
            if num_stocks < 1:
                num_stocks = 1
            if num_stocks > total_codes:
                num_stocks = total_codes
            import random
            selected_indices = random.sample(range(total_codes), num_stocks)
            selected_codes = unique_codes[selected_indices]
            print(f"{len(selected_codes)}ê°œ ì¢…ëª© ëœë¤ ì„ íƒë¨")
        except Exception as e:
            print(f"ëœë¤ ì„ íƒ ì˜¤ë¥˜: {e}. ëª¨ë“  ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return filtered_results
    
    else:
        print("ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ëª¨ë“  ì¢…ëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return filtered_results
    
    # ì„ íƒëœ ì¢…ëª©ë§Œ í•„í„°ë§
    filtered_subset = filtered_results[filtered_results['code_name'].isin(selected_codes)].copy()
    print(f"ì„ íƒëœ ì¢…ëª©ë“¤: {', '.join(selected_codes[:5])}{'...' if len(selected_codes) > 5 else ''}")
    print(f"ì´ {len(filtered_subset)}ê°œ ë°ì´í„° í¬ì¸íŠ¸ê°€ í›ˆë ¨ì— ì‚¬ìš©ë©ë‹ˆë‹¤.")
    
    return filtered_subset

def hybrid_split(X, y, test_size=0.2):
    # ë ˆì´ë¸”ì´ 0ì´ ì•„ë‹Œ ë°ì´í„° ì‹ë³„
    signal_indices = y[y != 0].index
    non_signal_indices = y[y == 0].index
    
    # ê°ê° 80/20ìœ¼ë¡œ ë¶„í• 
    train_signal = signal_indices[:int(len(signal_indices)*(1-test_size))]
    test_signal = signal_indices[int(len(signal_indices)*(1-test_size)):]

    train_non_signal = non_signal_indices[:int(len(non_signal_indices)*(1-test_size))]
    test_non_signal = non_signal_indices[int(len(non_signal_indices)*(1-test_size)):]

    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìƒì„±
    train_indices = list(train_signal) + list(train_non_signal)
    test_indices = list(test_signal) + list(test_non_signal)

    # ì¸ë±ìŠ¤ ì •ë ¬ (ì‹œê°„ ìˆœì„œ ìœ ì§€)
    train_indices.sort()
    test_indices.sort()

    # ë°ì´í„° ë¶„í• 
    X_train = X.loc[train_indices]
    X_test = X.loc[test_indices]
    y_train = y.loc[train_indices]
    y_test = y.loc[test_indices]

    return X_train, X_test, y_train, y_test

def load_filtered_stock_results(db_manager, table):
    # í•„í„°ë§ëœ ì¢…ëª©ì˜ featureì™€ label ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    try:
        # 1. í•„í„°ë§ëœ ì¢…ëª© ê²°ê³¼ ë¡œë“œ (ì¢…ëª©ëª…, signal_date, estimated_profit_rate)
        query = f"SELECT * FROM {table}"
        filtered_df = db_manager.execute_query(query)
        if (filtered_df.empty):
            print("No data loaded from filtered_stock_result table.")
            return pd.DataFrame()
        return filtered_df
        
        
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def load_daily_craw_data(db_manager, table, start_date, end_date):
    try:
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')
        print(f"Loading data from {start_date_str} to {end_date_str} for table {table}")
        
        query = f"""
            SELECT * FROM `{table}`
            WHERE date >= '{start_date_str}' AND date <= '{end_date_str}'
            ORDER BY date ASC
        """
        
        df = db_manager.execute_query(query)
        print(f"Data loaded from {start_date_str} to {end_date_str} for table {table}: {len(df)} rows")
        return df
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        return pd.DataFrame()

# ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œ
def load_daily_craw_data_batch(db_manager, table, validation_dates, start_date_offset=1200):
    """ì—¬ëŸ¬ ê²€ì¦ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œí•©ë‹ˆë‹¤.""" 
    try:
        min_date = min(validation_dates) - timedelta(days=start_date_offset)
        max_date = max(validation_dates)
        
        min_date_str = min_date.strftime('%Y%m%d')
        max_date_str = max_date.strftime('%Y%m%d')
        
        query = f"""
            SELECT * FROM `{table}` 
            WHERE date >= '{min_date_str}' AND date <= '{max_date_str}'
            ORDER BY date ASC
        """
        
        df = db_manager.execute_query(query)
        return df
    except Exception as e:
        print(f"Error loading batch data: {e}")
        return pd.DataFrame()

def label_data(df, valid_signal_dates, estimated_profit_rates):
    # Check for empty lists
    if not valid_signal_dates or not estimated_profit_rates:
        print("Warning: valid_signal_dates or estimated_profit_rates is empty.")
        return df  # Return the DataFrame without labeling

    # Combine signal dates and profit rates into tuples
    signal_data = list(zip(valid_signal_dates, estimated_profit_rates))

    # Remove duplicates using set and convert back to list
    unique_signal_data = list(set(signal_data))

    # Sort by date
    unique_signal_data.sort(key=lambda x: x[0])

    # ë‚ ì§œ í˜•ì‹ì„ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ df['date']ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
    df['date'] = pd.to_datetime(df['date'])
    
    # ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì„¤ì •
    df['Label'] = 0
    df['Label'] = df['Label'].astype('float64')
    
    # signal_dataì— ìˆëŠ” ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í–‰ì— ëŒ€í•´ Labelì„ estimated_profit_rates ë¡œ ì„¤ì •
    for signal_date, profit_rate in unique_signal_data:
        # signal_dateë„ datetime ê°ì²´ë¡œ ë³€í™˜
        signal_date = pd.to_datetime(signal_date)
        df.loc[df['date'] == signal_date, 'Label'] = profit_rate
    
    print(f"Number of unique signal dates after removing duplicates: {len(unique_signal_data)}")
    print(f"Number of labeled points: {(df['Label'] != 0).sum()}")
    
    return df
 

# ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë” ë‹¨ìˆœí™”í•˜ê¸°
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(InputLayer(shape=input_shape))
    # ë” ì‘ì€ ë„¤íŠ¸ì›Œí¬, ë” ê°•í•œ ì •ê·œí™”
    model.add(LSTM(2, kernel_regularizer=l2(0.05)))  # ìœ ë‹› ìˆ˜ë¥¼ 2ê°œë¡œ ë” ê°ì†Œ
    model.add(BatchNormalization())  # ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€
    model.add(Dropout(0.9))  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì¦ê°€
    model.add(Dense(1))
    
    # í•™ìŠµë¥  ì¡°ì •
    optimizer = Adam(learning_rate=0.000005)  # í•™ìŠµë¥  ë” ê°ì†Œ
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

def create_lightweight_lstm_model(input_shape):
    """ë” ê°€ë²¼ìš´ LSTM ëª¨ë¸ ìƒì„±"""
    model = Sequential()
    model.add(InputLayer(shape=input_shape))
    # ìœ ë‹› ìˆ˜ ê°ì†Œ, ë ˆì´ì–´ ë‹¨ìˆœí™”
    model.add(LSTM(1, kernel_regularizer=l2(0.01)))
    model.add(Dense(1))
    
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

def create_improved_lstm_model(input_shape):
    model = Sequential()
    model.add(InputLayer(shape=input_shape))
    
    # 1ë‹¨ê³„: LSTM ìœ ë‹› ìˆ˜ ì¦ê°€ (2 â†’ 32)
    model.add(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01)))
    #model.add(BatchNormalization())
    
    # 2ë‹¨ê³„: ì ì ˆí•œ ë“œë¡­ì•„ì›ƒ (0.9 â†’ 0.3)
    model.add(Dropout(0.5))
    
    # 3ë‹¨ê³„: ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´ ì¶”ê°€
    model.add(LSTM(16, kernel_regularizer=l2(0.01)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    
    # 4ë‹¨ê³„: ì™„ì „ ì—°ê²° ë ˆì´ì–´ ì¶”ê°€
    model.add(Dense(8, activation='linear'))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    
    # í•™ìŠµë¥  ì¡°ì • - í˜„ì¬ 0.000005ëŠ” ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŒ
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

def create_advanced_lstm_model(input_shape):
    model = Sequential()
    model.add(InputLayer(shape=input_shape))
    
    # ìœ ë‹› ìˆ˜ë¥¼ ë” ì¦ê°€ (32 â†’ 64)
    model.add(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.005)))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    # ì¤‘ê°„ LSTM ë ˆì´ì–´ ì¶”ê°€
    model.add(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.005)))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    # ë§ˆì§€ë§‰ LSTM ë ˆì´ì–´
    model.add(LSTM(16, kernel_regularizer=l2(0.005)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    
    # ë” ë§ì€ Dense ë ˆì´ì–´ ì¶”ê°€
    model.add(Dense(16, activation='linear', kernel_regularizer=l2(0.005)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    
    model.add(Dense(8, activation='linear', kernel_regularizer=l2(0.005)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    
    model.add(Dense(1))
    
    # í•™ìŠµë¥  ì¦ê°€
    optimizer = Adam(learning_rate=0.0003)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

# CPUì— ìµœì í™”ëœ ê²½ëŸ‰ ëª¨ë¸
def create_cpu_optimized_model(input_shape):
    model = Sequential()
    model.add(InputLayer(shape=input_shape))
    model.add(LSTM(4, kernel_regularizer=l2(0.01)))  # ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •
    model.add(BatchNormalization())
    model.add(Dropout(0.5))  # ê³¼ì í•© ë°©ì§€
    model.add(Dense(1))
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

def train_lstm_model(X, y):
    try:
        print('Training LSTM model')
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        y = y[X.index]
        
        # ë°ì´í„° í˜•íƒœ ë³€í™˜ (LSTM ì…ë ¥ í˜•íƒœì— ë§ê²Œ)
        X = np.expand_dims(X.values, axis=2)
        
        # LSTM ëª¨ë¸ ìƒì„±
        model = create_improved_lstm_model((X.shape[1], X.shape[2]))
        
        # ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            min_delta=0.001,
            restore_best_weights=True,
            verbose=1
        )
        
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = {0: len(y) / (2 * (len(y) - sum(y))), 
                        1: len(y) / (2 * sum(y))}

        # ëª¨ë¸ í›ˆë ¨ ì‹œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        history = model.fit(X, y, epochs=100, batch_size=32, 
                           validation_split=0.2, 
                           class_weight=class_weights,
                           callbacks=[early_stopping])
        
        return model
    except Exception as e:
        print(f'Error training LSTM model: {e}')
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ traceback ì •ë³´ ì¶œë ¥
        return None
    

def train_improved_lstm_model(filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id, code_name, current_idx=None, total_codes=None):
    try:
        print(f'Training improved LSTM model for {code_name}')
        if filtered_results.empty:
            print("Filtered results are empty. Cannot train model.")
            return None
        X = filtered_results[COLUMNS_TRAINING_DATA]
        y = filtered_results['Label']
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        y = y[X.index]
        print("Data loaded for LSTM training:")
        print(X.tail())
        print(y.tail())
        
        # TimeSeriesSplit ì‚¬ìš©
        tscv = TimeSeriesSplit(n_splits=5, test_size=len(X)//10)
        best_model = None
        best_val_loss = float('inf')
        
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # ë°ì´í„° í˜•íƒœ ë³€í™˜ (LSTM ì…ë ¥ í˜•íƒœì— ë§ê²Œ)
            X_train = np.expand_dims(X_train.values, axis=2)
            X_val = np.expand_dims(X_val.values, axis=2)
            
            # ëª¨ë¸ ìƒì„±
            model = create_improved_lstm_model((X_train.shape[1], X_train.shape[2]))
            
            # ì½œë°± ì„¤ì •
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=5,
                min_delta=0.001,
                restore_best_weights=True,
                verbose=1
            )
            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=0.00001,
                verbose=1
            )
            
            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë¶€ë¶„ ìˆ˜ì • - ì§„í–‰ ìƒí™© ì •ë³´ í¬í•¨
            pos_samples = sum(y_train > 0)
            if pos_samples > 0 and pos_samples < len(y_train):
                class_weights = {
                    0: len(y_train) / (2 * (len(y_train) - pos_samples)), 
                    1: len(y_train) / (2 * pos_samples)
                }
            else:
                # í•œìª½ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš© + ì§„í–‰ ìƒí™© í‘œì‹œ
                idx_info = f" ({current_idx+1}/{total_codes})" if current_idx is not None and total_codes is not None else ""
                print(f"Warning: Only one class present in training data for {code_name}{idx_info}. Using default weights.")
                class_weights = {0: 1.0, 1: 1.0}
            
            # ëª¨ë¸ í›ˆë ¨
            history = model.fit(
                X_train, y_train, 
                epochs=100,
                batch_size=64,
                validation_data=(X_val, y_val),
                class_weight=class_weights,
                callbacks=[early_stopping, reduce_lr],
                verbose=1
            )
            
            # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥
            val_loss = min(history.history['val_loss'])
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model = model
        
        return best_model
    
    except Exception as e:
        print(f'Error training LSTM model: {e}')
        import traceback
        traceback.print_exc()
        return None

def train_continued_lstm_model(filtered_results, previous_model, code_name, current_idx, total_codes):
    try:
        print(f'Continuing training of model with {code_name} data')
        X = filtered_results[COLUMNS_TRAINING_DATA]
        y = filtered_results['Label']
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        y = y[X.index]
        
        # ë°ì´í„° í˜•íƒœ ë³€í™˜
        X_reshaped = np.expand_dims(X.values, axis=2)
        
        # ê¸°ì¡´ ëª¨ë¸ë¡œ ê³„ì† í›ˆë ¨
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            min_delta=0.001,
            restore_best_weights=True,
            verbose=1
        )
        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=0.00001,
            verbose=1
        )
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        pos_samples = sum(y)
        if pos_samples > 0 and pos_samples < len(y):
            class_weights = {
                0: len(y) / (2 * (len(y) - pos_samples)), 
                1: len(y) / (2 * pos_samples)
            }
        else:
            idx_info = f" ({current_idx+1}/{total_codes})" if current_idx is not None else ""
            print(f"Warning: Only one class present in training data for {code_name}{idx_info}. Using default weights.")
            class_weights = {0: 1.0, 1: 1.0}
        
        # ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í›ˆë ¨
        history = previous_model.fit(
            X_reshaped, y, 
            epochs=50,  # ì ì€ ì—í¬í¬ë¡œ ì¶”ê°€ í›ˆë ¨
            batch_size=64,
            validation_split=0.2,
            class_weight=class_weights,
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )
        
        # ì¶”ê°€ í›ˆë ¨ëœ ëª¨ë¸ ë°˜í™˜
        return previous_model
        
    except Exception as e:
        print(f'Error continuing training: {e}')
        import traceback
        traceback.print_exc()
        return previous_model  # ì˜¤ë¥˜ ì‹œ ì›ë˜ ëª¨ë¸ ë°˜í™˜

def evaluate_lstm_model_with_tss(model, X, y, n_splits=3):
    tss = TimeSeriesSplit(n_splits=n_splits)
    mse_scores = []
    mae_scores = []
    for train_index, test_index in tqdm(tss.split(X), total=n_splits, desc="Evaluating with TimeSeriesSplit"):
        if len(train_index) > 10000:
            train_index = train_index[-10000:]
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index]
        X_train_reshaped = np.expand_dims(X_train.values, axis=2)
        X_test_reshaped = np.expand_dims(X_test.values, axis=2)
        model = create_lstm_model((X_train_reshaped.shape[1], X_train_reshaped.shape[2]))
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, restore_best_weights=True, verbose=1)
        history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=16, 
                            validation_split=0.2, callbacks=[early_stopping])
        import gc
        gc.collect()
        y_pred = model.predict(X_test_reshaped)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse_scores.append(mse)
        mae_scores.append(mae)
        print(f"TimeSeriesSplit Fold - MSE: {mse:.4f}, MAE: {mae:.4f}")
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    print(f"TimeSeriesSplit - Average MSE: {np.mean(mse_scores):.4f}, Average MAE: {np.mean(mae_scores):.4f}")
    return np.mean(mse_scores), np.mean(mae_scores)

# ëª¨ë¸ í‰ê°€ ë¶€ë¶„ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½
def evaluate_lstm_model(model, X, y):
    try:
        # TimeSeriesSplitì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í‰ê°€
        print("Evaluating with TimeSeriesSplit...")
        tss_mse, tss_mae = evaluate_lstm_model_with_tss(model, X, y)
        
        # TimeSeriesSplit ê²°ê³¼ ì¶œë ¥
        print(f"TimeSeriesSplit - MSE: {tss_mse:.4f}, MAE: {tss_mae:.4f}")
        
        # MAEê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸
        return -tss_mae
    except Exception as e:
        print(f"Error evaluating LSTM model: {e}")
        return -float('inf')  # ìµœì†Œê°’ ë°˜í™˜

# 2. tf.function ë°ì½”ë ˆì´í„° ì¶”ê°€
@tf.function(reduce_retracing=True)
def predict_batch(model, x):
    return model(x, training=False)

# predict_pattern í•¨ìˆ˜ ìˆ˜ì •
def predict_pattern(model, df, stock_code, use_data_dates=True):
    try:
        print('Predicting patterns')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'stock_code'])
            
        X = df[COLUMNS_TRAINING_DATA]
        # ë¬´í•œëŒ€ ê°’ì´ë‚˜ ë„ˆë¬´ í° ê°’ ì œê±°
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        
        # ì—¬ê¸°ì— X_reshaped ì •ì˜ ì¶”ê°€
        X_reshaped = np.expand_dims(X.values, axis=2)
        
        # ì¼ê´€ëœ ë°°ì¹˜ í¬ê¸° ìœ ì§€
        batch_size = 32
        predictions = []

        for i in range(0, len(X_reshaped), batch_size):
            batch = X_reshaped[i:i+batch_size]
            # ë°°ì¹˜ í¬ê¸° ì¼ì •í•˜ê²Œ ìœ ì§€
            if len(batch) < batch_size:
                batch_preds = model.predict(batch, verbose=0)
            else:
                batch_preds = predict_batch(model, batch)
            predictions.append(batch_preds)

        predictions = np.concatenate(predictions)
        
        df = df.loc[X.index]  # ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€
        df['Prediction'] = predictions
        
        # ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > cf.LSTM_PREDICTION_LIMIT).sum()} matches found')
        
        # ë‚ ì§œ í˜•ì‹ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        try:
            # MySQLì˜ YYYYMMDD í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            if df['date'].dtype == 'object':
                # YYYYMMDD í˜•ì‹ì˜ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # NaT ê°’ ì œê±°
            df = df.dropna(subset=['date'])
            print(f"Date range in data: {df['date'].min()} to {df['date'].max()}")
            
            # ê²€ì¦ ê¸°ê°„ ì„¤ì • ë¶€ë¶„ ìˆ˜ì •
            if use_data_dates:
                # ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œ ì´í›„ë¡œ ì˜ˆì¸¡ ê²€ì¦ ê¸°ê°„ ì„¤ì • (í›ˆë ¨ ì§í›„ ê²€ì¦ìš©)
                max_date = df['date'].max()
                validation_start_date = max_date + pd.Timedelta(days=1)
                validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
            else:
                # cf.pyì— ì„¤ì •ëœ ê²€ì¦ ê¸°ê°„ ì‚¬ìš© (ì™¸ë¶€ ê²€ì¦ìš©)
                validation_start_date = pd.to_datetime(str(cf.VALIDATION_START_DATE).zfill(8), format='%Y%m%d')
                validation_end_date = pd.to_datetime(str(cf.VALIDATION_END_DATE).zfill(8), format='%Y%m%d')
            
            print(f"Validation period: {validation_start_date} to {validation_end_date}")
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ íŒ¨í„´ í•„í„°ë§ (Predictionì´ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ)
            recent_patterns = df[
                (df['Prediction'] > cf.LSTM_PREDICTION_LIMIT) & 
                (df['date'] >= validation_start_date) & 
                (df['date'] <= validation_end_date)
            ].copy()
            
            print(f'Filtered patterns in validation period: {len(recent_patterns)}')
            
            if not recent_patterns.empty:
                recent_patterns['stock_code'] = stock_code
                result = recent_patterns[['date', 'stock_code']]
                print(f'Found patterns for {stock_code}:')
                print(result)
                return result
            else:
                print(f'No patterns found for {stock_code} in validation period')
                return pd.DataFrame(columns=['date', 'stock_code'])
                
        except Exception as e:
            print(f"Error in date processing: {e}")
            print(f"Debug info - df['date'] sample: {df['date'].head()}")
            print(f"Debug info - validation dates: {cf.VALIDATION_END_DATE}")
            return pd.DataFrame(columns=['date', 'stock_code'])
            
    except Exception as e:
        print(f'Error predicting patterns: {e}')
        print(f'Error type: {type(e).__name__}')
        import traceback
        print(f'Stack trace:\n{traceback.format_exc()}')
        return pd.DataFrame(columns=['date', 'stock_code'])

# ë” íš¨ìœ¨ì ì¸ ì˜ˆì¸¡ í•¨ìˆ˜
@tf.function(reduce_retracing=True)
def predict_batch(model, x):
    return model(x, training=False)

def predict_pattern_optimized(model, df, code_name, use_data_dates=True):
    try:
        print('Predicting patterns (optimized)')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
        X = df[COLUMNS_TRAINING_DATA]
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        if X.empty:
            print("Empty features data, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
        batch_size = 128
        X_reshaped = np.expand_dims(X.values, axis=2)
        try:
            if len(X_reshaped) <= 1000:
                predictions = model.predict(X_reshaped, batch_size=batch_size, verbose=0)
            else:
                predictions = np.zeros((len(X_reshaped), 1))
                for i in range(0, len(X_reshaped), batch_size):
                    end_idx = min(i + batch_size, len(X_reshaped))
                    batch = X_reshaped[i:end_idx]
                    predictions[i:end_idx] = model.predict(batch, verbose=0)
        except Exception as e:
            print(f"Error during prediction: {e}")
            return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
        df_result = df.loc[X.index].copy()
        df_result['Prediction'] = predictions
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > 0).sum()} matches found')
        try:
            if df_result['date'].dtype == 'object':
                df_result['date'] = pd.to_datetime(df_result['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df_result['date']):
                df_result['date'] = pd.to_datetime(df_result['date'], errors='coerce')
            df_result = df_result.dropna(subset=['date'])
        except Exception as e:
            print(f"Error processing dates: {e}")
            return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
        if use_data_dates:
            max_date = df_result['date'].max()
            validation_start_date = max_date + pd.Timedelta(days=1)
            validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
        else:
            validation_start_date = pd.to_datetime(str(cf.VALIDATION_START_DATE).zfill(8), format='%Y%m%d')
            validation_end_date = pd.to_datetime(str(cf.VALIDATION_END_DATE).zfill(8), format='%Y%m%d')
        print(f"Validation period: {validation_start_date} to {validation_end_date}")
        recent_patterns = df_result[
            (df_result['Prediction'] > 0) & 
            (df_result['date'] >= validation_start_date) & 
            (df_result['date'] <= validation_end_date)
        ].copy()
        print(f'Filtered patterns in validation period: {len(recent_patterns)}')
        if not recent_patterns.empty:
            recent_patterns['code_name'] = code_name
            result_df = recent_patterns[['date', 'code_name', 'Prediction']]
            print(f'Found patterns for {code_name}:')
            print(result_df)
            return result_df
        else:
            print(f'No patterns found for {code_name} in validation period')
            return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
    except Exception as e:
        print(f'Error in optimized prediction: {e}')
        import traceback
        traceback.print_exc()
        return pd.DataFrame(columns=['date', 'code_name', 'Prediction'])


def evaluate_performance_improved(df, start_date, end_date):
    """ìµœëŒ€ ìˆ˜ìµë¥ ê³¼ ìµœëŒ€ ì†ì‹¤ì„ ëª¨ë‘ ê³„ì‚°í•˜ëŠ” ê°œì„ ëœ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜"""
    try:
        print('Evaluating performance with risk metrics')
        df['date'] = pd.to_datetime(df['date'])
        df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
        
        if df.empty:
            print(f"No data found between {start_date} and {end_date}")
            return None, None, None
            
        # ì´ˆê¸° ì¢…ê°€ (ë§¤ìˆ˜ ê°€ê²©)
        initial_close = df['close'].iloc[0]
        
        # ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°
        df['daily_return'] = df['close'] / initial_close - 1
        
        # ìµœëŒ€ ìƒìŠ¹ë¥  ê³„ì‚°
        max_return = df['daily_return'].max() * 100
        max_return_day = df.loc[df['daily_return'].idxmax(), 'date']
        
        # ìµœëŒ€ í•˜ë½ë¥  ê³„ì‚°
        max_loss = df['daily_return'].min() * 100
        max_loss_day = df.loc[df['daily_return'].idxmin(), 'date']
        
        # ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  (ìµœëŒ€ ìƒìŠ¹ë¥  - ìµœëŒ€ í•˜ë½ë¥ ì˜ ì ˆëŒ€ê°’)
        risk_adjusted_return = max_return - abs(max_loss)
        
        result = {
            'max_return': max_return,
            'max_return_day': max_return_day,
            'max_loss': max_loss,
            'max_loss_day': max_loss_day,
            'risk_adjusted_return': risk_adjusted_return
        }
        
        return result
        
    except Exception as e:
        print(f'Error evaluating performance: {e}')
        import traceback
        traceback.print_exc()
        return None

# ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ë„ ìˆ˜ì •
def save_performance_to_db(df, db_manager, table):
    try:
        result = db_manager.to_sql(df, table)
        if result:
            print(f"Performance results saved to {table} table in {db_manager.database} database")
        return result
    except Exception as e:
        print(f"Error saving performance results to MySQL: {e}")
        return False

def validate_by_date_window(model, db_manager, stock_items, validation_start_date, validation_end_date):
    """ê° ë‚ ì§œë³„ë¡œ n-500 ~ në´‰ê¹Œì§€ ë°ì´í„°ë¡œ ê²€ì¦"""
    all_results = []
    
    # ê²€ì¦í•  ë‚ ì§œë“¤ ìƒì„±
    validation_days = (validation_end_date - validation_start_date).days + 1
    validation_dates = [validation_start_date + timedelta(days=i) for i in validation_days]
    
    print(f"ì´ {len(validation_dates)}ì¼ì— ëŒ€í•´ ê²€ì¦ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # ì „ì²´ ë‚ ì§œì— ëŒ€í•œ í”„ë¡œê·¸ë ˆìŠ¤ ë°”
    date_pbar = tqdm(validation_dates, desc="ë‚ ì§œë³„ ê²€ì¦", position=0, leave=True)
    
    total_patterns_found = 0
    
    for current_date in date_pbar:
        date_str = current_date.strftime('%Y%m%d')
        date_pbar.set_description(f"ë‚ ì§œ ê²€ì¦: {date_str}")
        
        # ì¢…ëª©ë³„ í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        stock_pbar = tqdm(enumerate(stock_items.itertuples()), 
                         total=len(stock_items), 
                         desc=f"{date_str} ì¢…ëª© ê²€ì¦",
                         position=1, 
                         leave=False)
        
        patterns_found_today = 0
        
        for idx, row in stock_pbar:
            table_name = row.code_name
            stock_pbar.set_postfix({'ì¢…ëª©': table_name, 'ë°œê²¬': patterns_found_today})
            
            # n-500 ~ në´‰ ë°ì´í„° ë¡œë“œ
            window_start_date = current_date - timedelta(days=2000)  # ì¶©ë¶„íˆ ê³¼ê±° ë°ì´í„° ë¡œë“œ (500ë´‰ í™•ë³´)
            window_end_date = current_date
            
            df = load_daily_craw_data(db_manager, table_name, window_start_date, window_end_date)
            
            if not df.empty and len(df) >= 250:
                # íŠ¹ì„± ì¶”ì¶œ
                df = extract_features(df)
                
                if not df.empty:
                    # ë§ˆì§€ë§‰ 500ë´‰ë§Œ ì‚¬ìš©
                    if len(df) > 500:
                        df = df.iloc[-500:].copy()
                    
                    # íŒ¨í„´ ì˜ˆì¸¡
                    result = predict_pattern_optimized(best_model, df, table_name, use_data_dates=False)

                    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ í™•ì¸
                    if isinstance(result, pd.DataFrame) and not result.empty:
                        # ê²°ê³¼ ì²˜ë¦¬
                        all_results.append(result)
                        patterns_found_today += len(result)
                        total_patterns_found += len(result)
                        stock_pbar.set_postfix({'ì¢…ëª©': table_name, 'ë°œê²¬': patterns_found_today})
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if idx % 100 == 0:
                clear_memory()
        
        # ë‚ ì§œë³„ ê²°ê³¼ ì—…ë°ì´íŠ¸
        date_pbar.set_postfix({'ë°œê²¬ íŒ¨í„´': patterns_found_today, 'ì´ ë°œê²¬': total_patterns_found})
        
        # ì¢…ëª© í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë‹«ê¸°
        stock_pbar.close()
    
    # ë‚ ì§œ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë‹«ê¸°
    date_pbar.close()
    
    print(f"\nê²€ì¦ ì™„ë£Œ: ì´ {total_patterns_found}ê°œ íŒ¨í„´ ë°œê²¬")
    return pd.DataFrame(all_results)


def analyze_top_performers_by_date(performance_df, top_n=5):
    """ë‚ ì§œë³„ë¡œ ìƒìœ„ ì„±ê³¼ë¥¼ ë³´ì¸ ì¢…ëª©ì„ ë¶„ì„"""
    try:
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
        performance_df['pattern_date'] = pd.to_datetime(performance_df['pattern_date'])
        date_grouped = performance_df.groupby(performance_df['pattern_date'].dt.date)
        
        results = []
        date_summaries = []
        
        # ê° ë‚ ì§œë³„ë¡œ ì²˜ë¦¬
        for date, group in date_grouped:
            # ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ê¸°ì¤€ ìƒìœ„ ì¢…ëª© ì„ íƒ
            top_stocks = group.nlargest(top_n, 'risk_adjusted_return')
            
            # ë‚ ì§œë³„ ìš”ì•½ í†µê³„
            date_summary = {
                'date': date,
                'total_patterns': len(group),
                'avg_risk_adjusted_return': group['risk_adjusted_return'].mean(),  # ì´ë¦„ ë³€ê²½
                'avg_max_return': group['max_return'].mean(),
                'avg_max_loss': group['max_loss'].mean(),
                'top_performer': top_stocks.iloc[0]['code_name'] if len(top_stocks) > 0 else None,  # stock_code â†’ code_name
                'top_return': top_stocks.iloc[0]['risk_adjusted_return'] if len(top_stocks) > 0 else None
            }
            
            date_summaries.append(date_summary)
            results.append({'date': date, 'top_stocks': top_stocks})
        
        return results, pd.DataFrame(date_summaries)
        
    except Exception as e:
        print(f'Error analyzing top performers: {e}')
        import traceback
        traceback.print_exc()
        return [], pd.DataFrame()

def load_validation_data(craw_db, stock_items, validation_chunks, best_model):
    validation_results = pd.DataFrame(columns=['date', 'code_name', 'Prediction'])
    processed_pairs = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ (ì¢…ëª©, ë‚ ì§œ) ìŒ ì¶”ì 
    suspended_stocks = []  # ì •ì§€ì¢…ëª© ëª©ë¡

    validation_start_date = validation_chunks[0]
    validation_end_date = validation_chunks[-1]

    for idx, row in tqdm(enumerate(stock_items.itertuples(index=True)), desc="ì¢…ëª© ê²€ì¦", total=len(stock_items)):
        code_name = row.code_name
        print(f"\nê²€ì¦ ì¤‘ì¸ ì¢…ëª©: {code_name}")
        try:
            # ì •ì§€ì¢…ëª© í™•ì¸ - ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„° ë¡œë“œ
            suspension_check_df = load_daily_craw_data(
                craw_db, 
                code_name, 
                validation_start_date, 
                validation_end_date
            )
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆ ë°ì´í„°ê°€ ìˆê³  ëª¨ë“  ê±°ë˜ëŸ‰ì´ 0ì¸ ê²½ìš° ì •ì§€ì¢…ëª©ìœ¼ë¡œ ê°„ì£¼
            if not suspension_check_df.empty:
                if len(suspension_check_df) >= 5 and all(volume == 0 for volume in suspension_check_df['volume']):
                    print(f"âš ï¸ {code_name} - ì •ì§€ì¢…ëª©ìœ¼ë¡œ ê°ì§€ë¨ (ê²€ì¦ ê¸°ê°„ ë™ì•ˆ ê±°ë˜ëŸ‰ 0)")
                    suspended_stocks.append(code_name)
                    continue  # ì •ì§€ì¢…ëª©ì´ë¯€ë¡œ ë‹¤ìŒ ì¢…ëª©ìœ¼ë¡œ ë„˜ì–´ê°

            # ê° ì²­í¬ì— ëŒ€í•´ í•œ ë²ˆë§Œ ë°ì´í„° ë¡œë“œ
            all_df = pd.DataFrame()
            for chunk_idx, (chunk_start, chunk_end) in enumerate(zip(validation_chunks[:-1], validation_chunks[1:])):
                window_start_date = chunk_start - timedelta(days=1200)
                
                # ì´ë¯¸ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ì§€ ì•ŠìŒ
                if not all_df.empty and len(all_df) >= 500:
                    continue
                
                df = load_daily_craw_data(craw_db, code_name, window_start_date, chunk_end)
                if not df.empty and len(df) >= 250:
                    if all_df.empty:
                        all_df = df
                    else:
                        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© ì‹œ ì¤‘ë³µ ì œê±°
                        all_df = pd.concat([all_df, df]).drop_duplicates(subset=['date'])
            
            # ê±°ë˜ëŸ‰ì´ ëª¨ë‘ 0ì´ë©´ ì •ì§€ì¢…ëª©ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê±´ë„ˆë›°ê¸°
            if not all_df.empty and len(all_df) >= 20:
                recent_data = all_df.sort_values(by='date').tail(20)
                if all(volume == 0 for volume in recent_data['volume']):
                    print(f"âš ï¸ {code_name} - ì •ì§€ì¢…ëª©ìœ¼ë¡œ ê°ì§€ë¨ (ìµœê·¼ 20ì¼ê°„ ê±°ë˜ëŸ‰ 0)")
                    suspended_stocks.append(code_name)
                    continue

            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•œ ë²ˆë§Œ íŠ¹ì„± ì¶”ì¶œ ë° ì˜ˆì¸¡
            if not all_df.empty and len(all_df) >= 250:
                df_features = extract_features(all_df)
                if not df_features.empty:
                    if len(df_features) > 500:
                        df_features = df_features.iloc[-500:].copy()
                    
                    result = predict_pattern_optimized(best_model, df_features, code_name, use_data_dates=False)
                    if isinstance(result, pd.DataFrame) and not result.empty:
                        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ë³‘í•©
                        if validation_results.empty:
                            validation_results = result
                        else:
                            # ì´ë¯¸ ìˆëŠ” (ì¢…ëª©, ë‚ ì§œ) ìŒ ì œì™¸í•˜ê³  ì¶”ê°€
                            for _, row in result.iterrows():
                                pair_key = (row['code_name'], row['date'])
                                if pair_key not in processed_pairs:
                                    validation_results = pd.concat([validation_results, pd.DataFrame([row])], ignore_index=True)
                                    processed_pairs.add(pair_key)
        except Exception as e:
            print(f"Error processing {code_name}: {e}")
    
    # ìµœì¢… ì¤‘ë³µ ì œê±°
    validation_results = validation_results.drop_duplicates(subset=['code_name', 'date'])
    return validation_results


def filter_top_n_per_date(validation_results, top_n_per_date=5):
    filtered_results = []
    
    if not validation_results.empty and 'date' in validation_results.columns:
        # ë¨¼ì € ì¤‘ë³µ í•­ëª© ì œê±° (ì¢…ëª©ëª…ê³¼ ë‚ ì§œ ê¸°ì¤€)
        validation_results = validation_results.drop_duplicates(subset=['code_name', 'date'])
        
        # ê·¸ í›„ ë‚ ì§œë³„ ìƒìœ„ Nê°œ ì¢…ëª© ì„ íƒ
        date_groups = validation_results.groupby(validation_results['date'].dt.date)
        for date, group in date_groups:
            sorted_group = group.sort_values(by='Prediction', ascending=False)
            top_n_stocks = sorted_group.head(top_n_per_date)
            filtered_results.append(top_n_stocks)
            
        if filtered_results:
            validation_results = pd.concat(filtered_results)
    
    return validation_results

def evaluate_performance(validation_results, craw_db):
    performance_results = []
    for index, row in tqdm(validation_results.iterrows(), total=len(validation_results), desc="ì„±ëŠ¥ í‰ê°€", position=0):
        code_name = row['code_name']
        pattern_date = row['date']
        prediction_value = row['Prediction']
        performance_start_date = pattern_date + pd.Timedelta(days=1)
        performance_end_date = performance_start_date + pd.Timedelta(days=60)
        df = load_daily_craw_data(craw_db, code_name, performance_start_date, performance_end_date)
        print(f"Evaluating performance for {code_name} from {performance_start_date} to {performance_end_date}: {len(df)} rows")
        if not df.empty:
            perf_result = evaluate_performance_improved(df, performance_start_date, performance_end_date)
            if perf_result is not None:
                result_dict = {
                    'code_name': code_name,
                    'pattern_date': pattern_date,
                    'prediction': prediction_value,
                    'start_date': performance_start_date,
                    'end_date': performance_end_date,
                    'max_return': perf_result['max_return'],
                    'max_return_day': perf_result['max_return_day'],
                    'max_loss': perf_result['max_loss'],
                    'max_loss_day': perf_result['max_loss_day'],
                    'risk_adjusted_return': perf_result['risk_adjusted_return']
                }
                performance_results.append(result_dict)
            else:
                print(f"No valid return found for {code_name} from {performance_start_date} to {performance_end_date}")
        else:
            print(f"No data loaded for {code_name} from {performance_start_date} to {performance_end_date}")
        if (index + 1) % 100 == 0 or (index + 1) == len(validation_results):
            print(f"Processed {index + 1}/{len(validation_results)} validation results")
    return pd.DataFrame(performance_results)


def send_validation_summary(validation_results, performance_df, telegram_token, telegram_chat_id, results_table, buy_list_db, model_name='lstm'):
    if not validation_results.empty and 'code_name' in validation_results.columns:
        unique_stock_codes = validation_results['code_name'].nunique()
        print(f"\nNumber of unique stock codes found during validation: {unique_stock_codes}")
    else:
        print("\nNo validation results found with code_name column")
        unique_stock_codes = 0

    message = f"Validation completed. Number of unique stock codes found during validation: {unique_stock_codes}"
    send_telegram_message(telegram_token, telegram_chat_id, message)

    if not performance_df.empty:
        print("\nPerformance results:")
        print(performance_df)

        save_performance_to_db(performance_df, buy_list_db, performance_table)

        message = f"Performance completed. {results_table}\nTotal performance: {len(performance_df)}"
        send_telegram_message(telegram_token, telegram_chat_id, message)

        top_performers, date_summary = analyze_top_performers_by_date(performance_df, top_n=5)

        print("\n===== ì „ì²´ ì„±ëŠ¥ ìš”ì•½ =====")
        print(f"ì´ ê²€ì¦ ì¢…ëª© ìˆ˜: {len(performance_df)}")
        print(f"í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {performance_df['max_return'].mean():.2f}%")
        print(f"í‰ê·  ìµœëŒ€ ì†ì‹¤ë¥ : {performance_df['max_loss'].mean():.2f}%")
        print(f"í‰ê·  ìœ„í—˜ì¡°ì • ìˆ˜ìµë¥ : {performance_df['risk_adjusted_return'].mean():.2f}%")
        print(f"ìœ„í—˜ì¡°ì • ìˆ˜ìµë¥  ì¤‘ì•™ê°’: {performance_df['risk_adjusted_return'].median():.2f}%")

        print("\n===== ë‚ ì§œë³„ ì„±ëŠ¥ ìš”ì•½ =====")
        print(date_summary.sort_values(by='avg_risk_adjusted_return', ascending=False))
        
        print("\n===== ìœ„í—˜ì¡°ì • ìˆ˜ìµë¥  ê¸°ì¤€ ìƒìœ„ 5ê°œ ì¢…ëª© =====")
        top5_overall = performance_df.nlargest(5, 'risk_adjusted_return')
        print(top5_overall[['code_name', 'pattern_date', 'max_return', 'max_loss', 'risk_adjusted_return']])

        print("\n===== ë‚ ì§œë³„ ìµœê³  ì¢…ëª© =====")
        for result in top_performers:
            date = result['date']
            top_stocks = result['top_stocks']

            if not top_stocks.empty:
                print(f"\në‚ ì§œ: {date} - ìƒìœ„ {len(top_stocks)}ê°œ ì¢…ëª©")
                print(top_stocks[['code_name', 'max_return', 'max_loss', 'risk_adjusted_return']])

        print("\n===== ë‚ ì§œë³„ Prediction ê°’ ê¸°ì¤€ ìƒìœ„ 5ê°œ ì¢…ëª© ì„±ê³¼ =====")
        performance_df['pattern_date'] = pd.to_datetime(performance_df['pattern_date'])
        pred_date_groups = performance_df.groupby(performance_df['pattern_date'].dt.date)
        
        # ëª¨ë“  ë‚ ì§œì˜ ìƒìœ„ ì˜ˆì¸¡ ì¢…ëª©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_top_predictions = []

        for date, group in pred_date_groups:
            top_by_prediction = group.sort_values(by='prediction', ascending=False).head(5)

            if not top_by_prediction.empty:
                print(f"\në‚ ì§œ: {date} - Prediction ê¸°ì¤€ ìƒìœ„ 5ê°œ ì¢…ëª©")
                print(top_by_prediction[['code_name', 'prediction', 'max_return', 'max_loss', 'risk_adjusted_return']])
                
                # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
                message = f"ğŸ“Š ë‚ ì§œ: {date} - LSTM ì˜ˆì¸¡ ìƒìœ„ 5ê°œ ì¢…ëª©\n"
                for idx, row in top_by_prediction.iterrows():
                    message += f"{row['code_name']}: ì‹ ë¢°ë„ {row['prediction']:.4f}, ì˜ˆìƒìˆ˜ìµë¥  {row['max_return']:.2f}%\n"
                send_telegram_message(telegram_token, telegram_chat_id, message)
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
                db_data = top_by_prediction.copy()
                db_data['date'] = date  # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
                all_top_predictions.append(db_data)
        
        # ëª¨ë“  ë‚ ì§œì˜ ìƒìœ„ ì˜ˆì¸¡ ì¢…ëª©ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
        if all_top_predictions:
            all_predictions_df = pd.concat(all_top_predictions)
            
            # ì „ë‹¬ë°›ì€ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            print(f"Saving prediction results with model name: {model_name}")
            
            # deep_learning í…Œì´ë¸”ì— ì €ì¥ (ëª¨ë¸ ì´ë¦„ ì „ë‹¬)
            save_lstm_predictions_to_db(buy_list_db, all_predictions_df, model_name)
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
        try:
            if len(performance_df) > 1:
                correlation = performance_df['prediction'].corr(performance_df['max_return'])
                risk_adj_correlation = performance_df['prediction'].corr(performance_df['risk_adjusted_return'])
                print(f"\nì˜ˆì¸¡ê°’-ìµœëŒ€ìˆ˜ìµë¥  ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
                print(f"ì˜ˆì¸¡ê°’-ìœ„í—˜ì¡°ì •ìˆ˜ìµë¥  ìƒê´€ê³„ìˆ˜: {risk_adj_correlation:.4f}")
            else:
                print("\në°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"Error calculating correlation: {e}")

        # ... ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€ ...


def save_lstm_predictions_to_db(db_manager, predictions_df, model_name=None):
    """LSTM ì˜ˆì¸¡ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ê³  í…Œì´ë¸” í˜•ì‹ì— ë§ê²Œ ì»¬ëŸ¼ëª… ë³€ê²½
        dl_data = predictions_df[['date', 'code_name', 'prediction', 'max_return']].copy()
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • (ì œê³µëœ ì´ë¦„ì´ ì—†ìœ¼ë©´ 'lstm' ì‚¬ìš©)
        method_name = model_name if model_name else 'lstm'
        dl_data['method'] = method_name
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        dl_data = dl_data.rename(columns={
            'prediction': 'confidence',
            'max_return': 'estimated_profit_rate'
        })
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (DBConnectionManagerì˜ to_sql ë©”ì†Œë“œì— ë§ê²Œ ìˆ˜ì •)
        result = db_manager.to_sql(dl_data, 'deep_learning')  # if_existsì™€ index íŒŒë¼ë¯¸í„° ì œê±°
        if result:
            print(f"âœ… {len(dl_data)}ê°œ {method_name} ì˜ˆì¸¡ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return result
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_validation(best_model, buy_list_db, craw_db, results_table, current_date, model_name='lstm'):
    print(f"\nLoading data for validation from {cf.VALIDATION_START_DATE} to {cf.VALIDATION_END_DATE}")
    validation_start_date = pd.to_datetime(str(cf.VALIDATION_START_DATE).zfill(8), format='%Y%m%d')
    validation_end_date = pd.to_datetime(str(cf.VALIDATION_END_DATE).zfill(8), format='%Y%m%d')

    stock_items = get_stock_items(host, user, password, database_buy_list)
    total_stock_items = len(stock_items)
    print(f"\nì „ì²´ ì¢…ëª© ìˆ˜: {total_stock_items}")
    print(f"ê²€ì¦ ê¸°ê°„: {validation_start_date} ~ {validation_end_date}")
    print(stock_items.head())

    validation_chunks = [validation_start_date + timedelta(days=i) for i in range(0, (validation_end_date - validation_start_date).days + 1, 7)]
    if validation_end_date not in validation_chunks:
        validation_chunks.append(validation_end_date)

    validation_results = load_validation_data(craw_db, stock_items, validation_chunks, best_model)
    validation_results = filter_top_n_per_date(validation_results, top_n_per_date=5)
    performance_df = evaluate_performance(validation_results, craw_db)
    
    # model_name ì¸ì ì¶”ê°€ë¡œ ì „ë‹¬
    send_validation_summary(validation_results, performance_df, telegram_token, telegram_chat_id, results_table, buy_list_db, model_name)

def get_user_choice():
    while True:
        choice = input("Do you want to retrain the model? (yes/new/continue/validate/summary/no): ").strip().lower()
        if choice in ['yes', 'new', 'continue', 'validate', 'summary', 'no']:
            return choice
        else:
            print("Invalid choice. Please enter 'yes', 'new', 'continue', 'validate', 'summary', or 'no'.")

def load_model_and_validate(model_dir, buy_list_db, craw_db, results_table, current_date):
    try:
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        if model_files:
            print("Available model files:")
            for i, file in enumerate(model_files):
                print(f"{i + 1}. {file}")
                
            model_choice = int(input("Select a model to validate (number): ")) - 1
            if 0 <= model_choice < len(model_files):
                model_file = os.path.join(model_dir, model_files[model_choice])
                best_model = tf.keras.models.load_model(model_file)
                print(f"Loaded model from {model_file}")
                
                # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ - íŒŒì¼ ì´ë¦„ì—ì„œ .keras ì œê±°
                model_name = os.path.basename(model_file).replace('.keras', '')
                print(f"Using model name: {model_name}")
                
                run_validation(best_model, buy_list_db, craw_db, results_table, current_date, model_name)
            else:
                print("Invalid choice. Exiting.")
        else:
            print("No model files found in the directory.")
    except Exception as e:
        print(f"Error loading model: {e}")


def process_model_workflow(filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id):
    """ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ëª¨ë¸ í›ˆë ¨, ê³„ì† í›ˆë ¨, ê²€ì¦ ë˜ëŠ” ëª¨ë¸ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("Filtered stock results loaded successfully")
    
    if filtered_results.empty:
        print("Filtered results are empty. Exiting.")
        return
    
    choice = get_user_choice()
    
    if choice == 'yes' or choice == 'new':
        # ì¢…ëª© ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€
        select_option = input("ëª¨ë“  ì¢…ëª©ì„ í›ˆë ¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if select_option == 'n':
            filtered_results = select_stocks_for_training(filtered_results)
            
        # ìƒˆë¡œìš´ ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¬´ì‹œ
        process_filtered_results(filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)
    
    elif choice == 'continue':
        # ì¢…ëª© ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€
        select_option = input("ëª¨ë“  ì¢…ëª©ì„ ê³„ì† í›ˆë ¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        selected_filtered_results = filtered_results
        
        if select_option == 'n':
            selected_filtered_results = select_stocks_for_training(filtered_results)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
        checkpoint_files = [f for f in os.listdir('.') if f.startswith('training_checkpoint_') and f.endswith('.pkl')]
        
        if checkpoint_files:
            print("Available checkpoint files:")
            for i, file in enumerate(checkpoint_files):
                print(f"{i + 1}. {file}")
                
            try:
                checkpoint_choice = int(input("Select a checkpoint to continue from (number): ")) - 1
                if 0 <= checkpoint_choice < len(checkpoint_files):
                    training_checkpoint_file = checkpoint_files[checkpoint_choice]
                    checkpoint = load_training_checkpoint(training_checkpoint_file)
                    
                    if checkpoint:
                        print("Successfully loaded model from checkpoint")
        
                        # process_filtered_results ëŒ€ì‹  continue_training_from_checkpoint í˜¸ì¶œ
                        best_model = continue_training_from_checkpoint(
                            checkpoint, selected_filtered_results, buy_list_db, craw_db, 
                            model_dir, results_table, current_date, telegram_token, telegram_chat_id
                        )
                        
                        # í›ˆë ¨ í›„ ê²€ì¦ ì‹¤í–‰
                        run_validation(best_model, buy_list_db, craw_db, results_table, current_date)
                    else:
                        print("Failed to load checkpoint. Starting new training.")
                        process_filtered_results(selected_filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)
                else:
                    print("Invalid choice. Starting new training.")
                    process_filtered_results(selected_filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)
            except ValueError:
                print("Invalid input. Starting new training.")
                process_filtered_results(selected_filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)
        else:
            print("No checkpoint files found. Starting new training.")
            process_filtered_results(selected_filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)
    
    elif choice == 'validate':
        load_model_and_validate(model_dir, buy_list_db, craw_db, results_table, current_date)
    
    elif choice == 'no':
        print("Exiting without training.")
    
    elif choice == 'summary':
        # ëª¨ë¸ ìš”ì•½ ì •ë³´ ì¶œë ¥
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.keras')]
        if model_files:
            print("Available model files:")
            for i, file in enumerate(model_files):
                print(f"{i + 1}. {file}")
                
            model_choice = int(input("Select a model to summarize (number): ")) - 1
            if 0 <= model_choice < len(model_files):
                model_file = os.path.join(model_dir, model_files[model_choice])
                best_model = tf.keras.models.load_model(model_file)
                print(f"Loaded model from {model_file}")
                print_model_summary(best_model)
            else:
                print("Invalid choice. Exiting.")
        else:
            print("No model files found in the directory.")

def inspect_table_structure(db_manager, table):
    try:
        # í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        query = f"DESCRIBE {table}"
        structure = db_manager.execute_query(query)
        print(f"\nTable structure for {table}:")
        print(structure)
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        query = f"SELECT * FROM {table} LIMIT 5"
        sample = db_manager.execute_query(query)
        print(f"\nSample data from {table}:")
        print(sample)
        
        return structure, sample
    except Exception as e:
        print(f"Error inspecting table structure: {e}")
        return None, None

def load_data_for_lstm(db_manager, start_date, end_date):
    """ì¼ë³„ ì£¼ê°€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ LSTM ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±"""
    try:
        # ì£¼ì‹ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        stock_items = get_stock_items(host, user, password, database_buy_list)
        print(f"ì´ {len(stock_items)} ì¢…ëª© ì¤‘ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        all_data = []
        
        # ê° ì¢…ëª©ë³„ ë°ì´í„° ë¡œë“œ ë° ê°€ê³µ
        for i, row in tqdm(enumerate(stock_items.itertuples()), total=len(stock_items), desc="ë°ì´í„° ë¡œë“œ"):
            code_name = row.code_name
            
            # ì¼ë³„ ë°ì´í„° ë¡œë“œ
            df = load_daily_craw_data(db_manager, code_name, start_date, end_date)
            
            if not df.empty and len(df) >= 250:
                # íŠ¹ì„± ì¶”ì¶œ
                df_features = extract_features(df)
                
                if not df_features.empty:
                    # í•„ìš”í•œ ê²½ìš° ë¼ë²¨ ì¶”ê°€
                    df_features['stock_code'] = code_name
                    all_data.append(df_features)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ì •ë¦¬
            if i % 100 == 0:
                clear_memory()
        
        if all_data:
            combined_data = pd.concat(all_data)
            print(f"ì´ {len(combined_data)} í–‰ì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return combined_data
        else:
            print("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def load_stock_data_for_signals(db_manager, stock_signals, table):
    try:
        # ì‹ í˜¸ ë‚ ì§œì™€ ì˜ˆìƒ ìˆ˜ìµë¥  ì¶”ì¶œ
        valid_signal_dates = stock_signals['signal_date'].tolist()
        estimated_profit_rates = stock_signals['estimated_profit_rate'].tolist()
        
        # ê°€ì¥ ë¹ ë¥¸ ì‹ í˜¸ ë‚ ì§œì™€ ê°€ì¥ ëŠ¦ì€ ì‹ í˜¸ ë‚ ì§œ ì°¾ê¸°
        earliest_signal_date = pd.to_datetime(min(valid_signal_dates))
        latest_signal_date = pd.to_datetime(max(valid_signal_dates))
        
        # 1200ì¼ ì „ë¶€í„° ë°ì´í„°ë¥¼ ë¡œë“œ
        start_date = earliest_signal_date - pd.Timedelta(days=1200)
        end_date = latest_signal_date
        
        print(f"Loading data for {table} from {start_date} to {end_date}")
        
        # ë°ì´í„° ë¡œë“œ
        df = load_daily_craw_data(db_manager, table, start_date, end_date)
        
        if df.empty:
            print(f"No data loaded for {table} from {start_date} to {end_date}")
            return pd.DataFrame()
        
        print(f"Data loaded for {table}: {len(df)} rows")
        
        # íŠ¹ì„± ì¶”ì¶œ
        df_features = extract_features(df)
        
        if df_features.empty:
            print(f"No features extracted for {table}")
            return pd.DataFrame()
        
        # ë ˆì´ë¸” ë¶€ì—¬
        df_labeled = label_data(df_features, valid_signal_dates, estimated_profit_rates)
        
        print(f"Data for {table}: {len(df_labeled)} rows with features and labels")
        
        return df_labeled
    
    except Exception as e:
        print(f"Error loading data for {table}: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def split_and_extract_groups(df_labeled):
    """ì‹ í˜¸ ë‚ ì§œê°€ 3ê°œì›” ì´ìƒ ì°¨ì´ë‚˜ëŠ” ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ê·¸ë£¹ì˜ ë§ˆì§€ë§‰ ë‚ ì§œë¡œë¶€í„° ì´ì „ 500ë´‰ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # ë‚ ì§œ í˜•ì‹ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df_labeled['date'] = pd.to_datetime(df_labeled['date'])
        
        # ì‹ í˜¸ ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ í•„í„°ë§
        signal_dates = df_labeled[df_labeled['Label'] != 0]['date'].sort_values().unique()
        
        # ê·¸ë£¹í™”
        groups = []
        current_group = []
        
        for i, date in enumerate(signal_dates):
            if not current_group:
                current_group.append(date)
            else:
                last_date = current_group[-1]
                if (date - last_date).days > 90:  # 3ê°œì›” ì´ìƒ ì°¨ì´
                    groups.append(current_group)
                    current_group = [date]
                else:
                    current_group.append(date)
        
        if current_group:
            groups.append(current_group)
        
        print(f"Found {len(groups)} groups based on signal dates")
        
        # ê° ê·¸ë£¹ì˜ ë§ˆì§€ë§‰ ë‚ ì§œë¡œë¶€í„° ì´ì „ 500ë´‰ ê°€ì ¸ì˜¤ê¸°
        #all_group_data = []
        
        for i, group in enumerate(groups):
            last_date = group[-1]
            df_group = df_labeled[df_labeled['date'] <= last_date].copy()
            
            if len(df_group) > 500:
                df_group = df_group.iloc[-500:]
            return df_group
        # ë³‘í•©í•˜ì§€ ë§ê³  í•˜ë‚˜ì”© ë°˜í™˜í•˜ì—¬ íŠ¸ë ˆì´ë‹ì¤€ë¹„

    
    except Exception as e:
        print(f"Error in split_and_extract_groups: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def process_filtered_results(filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id):
    """í•„í„°ë§ëœ ì¢…ëª© ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ LSTM ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    if not filtered_results.empty:
        trained_models = []
        best_model = None
        
        # ì¢…ëª©ë³„ë¡œ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        unique_codes = filtered_results['code_name'].unique()
        total_codes = len(unique_codes)
        print(f"Total unique stock codes: {total_codes}")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ 10ê°œ ì¢…ëª©ì”© ì²˜ë¦¬
        batch_size = 10
        for batch_idx in range(0, total_codes, batch_size):
            batch_codes = unique_codes[batch_idx:batch_idx + batch_size]
            
            for code_name in tqdm(batch_codes, desc=f"Processing batch {batch_idx//batch_size + 1}"):
                # ì½”ë“œëª…ì— í•´ë‹¹í•˜ëŠ” ì‹ í˜¸ ë°ì´í„° í•„í„°ë§
                stock_signals = filtered_results[filtered_results['code_name'] == code_name]
                print(f"\nProcessing {code_name} ({batch_idx + list(batch_codes).index(code_name) + 1}/{total_codes}): {len(stock_signals)} signals")
                
                # í•´ë‹¹ ì¢…ëª©ì˜ ë°ì´í„° ë¡œë“œ ë° ë¼ë²¨ë§
                df_labeled = load_stock_data_for_signals(craw_db, stock_signals, code_name)
                
                if not df_labeled.empty:
                    # ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³  500ë´‰ ë°ì´í„° ì¶”ì¶œ
                    df_500 = split_and_extract_groups(df_labeled)
                    print(f"Data loaded for {code_name}: {len(df_500)} rows")
                    
                    # ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•´ í•„ìš”ì—†ëŠ” ë°ì´í„° ì‚­ì œ
                    del df_labeled
                    
                    # LSTM ëª¨ë¸ í›ˆë ¨
                    current_idx = batch_idx + list(batch_codes).index(code_name)
                    best_model = train_improved_lstm_model(df_500, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id, code_name, current_idx, total_codes)
                    
                    # ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•´ í•„ìš”ì—†ëŠ” ë°ì´í„° ì‚­ì œ
                    # del df_500
                    
                    if best_model is not None:
                        trained_models.append(code_name)
                        print(f"Model training successful for {code_name}")
                        
                        # ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ ì¢…ëª©ì´ê±°ë‚˜ ì „ì²´ì˜ ë§ˆì§€ë§‰ ì¢…ëª©ì´ë©´ ëª¨ë¸ ì €ì¥
                        current_idx = batch_idx + list(batch_codes).index(code_name)
                        
                        if (current_idx + 1) % 10 == 0 or current_idx == total_codes - 1:
                            # ì´ì „ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì‚­ì œ
                            for old_file in os.listdir(model_dir):
                                if old_file.startswith("improved_lstm_model_batch_") and old_file.endswith(f"_{current_date}.keras"):
                                    try:
                                        os.remove(os.path.join(model_dir, old_file))
                                        print(f"Removed previous checkpoint file: {old_file}")
                                    except Exception as e:
                                        print(f"Could not remove old checkpoint file: {old_file}, {e}")
                            
                            # ìƒˆ ëª¨ë¸ ì €ì¥ - íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            model_file = os.path.join(model_dir, f"improved_lstm_model_batch_{len(trained_models)}_{current_date}_{timestamp}.keras")
                            best_model.save(model_file)
                            print(f"Model saved to {model_file} after processing {len(trained_models)} stocks")
                            
                            # ì²´í¬í¬ì¸íŠ¸ì— ëª¨ë¸ê³¼ ì²˜ë¦¬ëœ ì¢…ëª© ëª©ë¡ ì €ì¥
                            checkpoint_state = {
                                'model': best_model,
                                'trained_models': trained_models.copy(),
                                'current_date': current_date
                            }
                            save_training_checkpoint(checkpoint_state, f"training_checkpoint_{current_date}.pkl")
                            
                            # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€
                            message = f"ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(trained_models)}ê°œ ì¢…ëª© ì²˜ë¦¬ ({current_idx+1}/{total_codes})"
                            send_telegram_message(telegram_token, telegram_chat_id, message)
                            
                    else:
                        print(f"Model training failed for {code_name}")
                else:
                    print(f"No labeled data found for {code_name}")
                
                # ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                clear_memory()
            
            # ë°°ì¹˜ ì²˜ë¦¬ í›„ ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            tf.keras.backend.clear_session()
            
        # í›ˆë ¨ ì™„ë£Œ í›„...

def print_model_summary(model):
    """ëª¨ë¸ì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if model is not None:
        print("\n===== Model Summary =====")
        model.summary()
    else:
        print("No model available to summarize.")

def continue_training_from_checkpoint(checkpoint, filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id):
    """ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ê³„ì†í•©ë‹ˆë‹¤."""
    print("Continuing training from checkpoint...")
    
    best_model = checkpoint['model']
    already_trained_models = checkpoint.get('trained_models', [])
    print(f"Already trained {len(already_trained_models)} models: {', '.join(already_trained_models[:5])}{'...' if len(already_trained_models) > 5 else ''}")
    
    if not filtered_results.empty:
        trained_models = already_trained_models.copy()  # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ ëª©ë¡ìœ¼ë¡œ ì‹œì‘
        
        # ì¢…ëª©ë³„ë¡œ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        unique_codes = filtered_results['code_name'].unique()
        total_codes = len(unique_codes)
        
        for idx, code_name in enumerate(tqdm(unique_codes, desc="Processing stocks")):
            # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì€ ê±´ë„ˆë›°ê¸°
            if code_name in already_trained_models:
                print(f"Skipping already trained model: {code_name}")
                continue
            
            # ì½”ë“œëª…ì— í•´ë‹¹í•˜ëŠ” ì‹ í˜¸ ë°ì´í„° í•„í„°ë§
            stock_signals = filtered_results[filtered_results['code_name'] == code_name]
            print(f"\nProcessing {code_name} ({idx+1}/{total_codes}): {len(stock_signals)} signals")
            
            # í•´ë‹¹ ì¢…ëª©ì˜ ë°ì´í„° ë¡œë“œ ë° ë¼ë²¨ë§
            df_labeled = load_stock_data_for_signals(craw_db, stock_signals, code_name)
            
            if not df_labeled.empty:
                # ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³  500ë´‰ ë°ì´í„° ì¶”ì¶œ
                df_500 = split_and_extract_groups(df_labeled)
                print(f"Data loaded for {code_name}: {len(df_500)} rows")
                
                # LSTM ëª¨ë¸ í›ˆë ¨
                new_model = train_continued_lstm_model(df_500, best_model, code_name, idx, total_codes)
                if new_model is not None:
                    # ì´ì „ ëª¨ë¸ì„ ìƒˆ ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
                    best_model = new_model
                    trained_models.append(code_name)
                    print(f"Model training successful for {code_name}")
                    
                    # 10ê°œ ì¢…ëª©ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ ì¢…ëª©ì´ë©´ ëª¨ë¸ ì €ì¥
                    if ((len(trained_models) - len(already_trained_models)) % 10 == 0) or (idx == total_codes - 1):
                        # ì´ì „ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì‚­ì œ
                        for old_file in os.listdir(model_dir):
                            if old_file.startswith("improved_lstm_model_continued_") and old_file.endswith(f"_{current_date}.keras"):
                                try:
                                    os.remove(os.path.join(model_dir, old_file))
                                    print(f"Removed previous checkpoint file: {old_file}")
                                except Exception as e:
                                    print(f"Could not remove old checkpoint file: {old_file}, {e}")
                        
                        # ìƒˆ ëª¨ë¸ ì €ì¥ - íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        model_file = os.path.join(model_dir, f"improved_lstm_model_continued_{len(trained_models)}_{current_date}_{timestamp}.keras")
                        best_model.save(model_file)
                        print(f"Model saved to {model_file} after processing {len(trained_models)} stocks")
                        
                        # ì²´í¬í¬ì¸íŠ¸ì— ëª¨ë¸ê³¼ ì²˜ë¦¬ëœ ì¢…ëª© ëª©ë¡ ì €ì¥
                        checkpoint_state = {
                            'model': best_model,
                            'trained_models': trained_models.copy(),
                            'current_date': current_date
                        }
                        save_training_checkpoint(checkpoint_state, f"training_checkpoint_{current_date}.pkl")
                        
                        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€
                        message = f"ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(trained_models)}ê°œ ì¢…ëª© ì²˜ë¦¬ ({idx+1}/{total_codes})"
                        send_telegram_message(telegram_token, telegram_chat_id, message)
                else:
                    print(f"Model training failed for {code_name}")
            else:
                print(f"No labeled data found for {code_name}")
        
        # í›ˆë ¨ ì™„ë£Œ í›„ í•œ ë²ˆë§Œ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡
        if len(trained_models) > len(already_trained_models):
            message = f"ëª¨ë¸ ì¶”ê°€ í›ˆë ¨ ì™„ë£Œ: {len(trained_models)}ê°œ ì¢…ëª© ({', '.join(trained_models[:5])}{'...' if len(trained_models) > 5 else ''})"
            send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return best_model


def extract_features(df):
    try:
        original_len = len(df)
        print(f'Original data rows: {original_len}')
        print('Extracting features')

        # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        if original_len < 250:  # ìµœì†Œ í•„ìš” ë°ì´í„° ìˆ˜
            print(f"Warning: Not enough data rows ({original_len}). Minimum 250 required.")
            return pd.DataFrame()

        # í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
        df = df[COLUMNS_CHART_DATA].copy()
        # ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
        df = df.sort_values(by='date')

        # ì´ë™í‰ê·  ê³„ì‚°
        df['MA5'] = df['close'].rolling(window=5).mean()
        df['MA10'] = df['close'].rolling(window=10).mean()
        df['MA20'] = df['close'].rolling(window=20).mean()
        df['MA60'] = df['close'].rolling(window=60).mean()
        df['MA120'] = df['close'].rolling(window=120).mean()
        df['MA240'] = df['close'].rolling(window=240).mean()

        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ ì²˜ë¦¬ ê°•í™”
        epsilon = 1e-10  # ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ 0 ëŒ€ì²´
        df['MA5'] = df['MA5'].replace(0, epsilon)
        df['MA10'] = df['MA10'].replace(0, epsilon)
        df['MA20'] = df['MA20'].replace(0, epsilon)
        df['MA60'] = df['MA60'].replace(0, epsilon)
        df['MA120'] = df['MA120'].replace(0, epsilon)
        df['MA240'] = df['MA240'].replace(0, epsilon)

        # ê° íŠ¹ì„± ê³„ì‚° í›„ ë‚¨ì€ ë°ì´í„° ë””ë²„ê¹…
        df['Close_to_MA5'] = df['close'] / df['MA5']
        df['Close_to_MA10'] = df['close'] / df['MA10']
        df['Close_to_MA20'] = df['close'] / df['MA20']
        df['Close_to_MA60'] = df['close'] / df['MA60']
        df['Close_to_MA120'] = df['close'] / df['MA120']
        df['Close_to_MA240'] = df['close'] / df['MA240']

        df['Volume_MA5'] = df['volume'].rolling(window=5).mean().replace(0, epsilon)
        df['Volume_MA10'] = df['volume'].rolling(window=10).mean().replace(0, epsilon)
        df['Volume_MA20'] = df['volume'].rolling(window=20).mean().replace(0, epsilon)
        df['Volume_MA60'] = df['volume'].rolling(window=60).mean().replace(0, epsilon)
        df['Volume_MA120'] = df['volume'].rolling(window=120).mean().replace(0, epsilon)
        df['Volume_MA240'] = df['volume'].rolling(window=240).mean().replace(0, epsilon)

        df['Volume_to_MA5'] = df['volume'] / df['Volume_MA5']
        df['Volume_to_MA10'] = df['volume'] / df['Volume_MA10']
        df['Volume_to_MA20'] = df['volume'] / df['Volume_MA20']
        df['Volume_to_MA60'] = df['volume'] / df['Volume_MA60']
        df['Volume_to_MA120'] = df['volume'] / df['Volume_MA120']
        df['Volume_to_MA240'] = df['volume'] / df['Volume_MA240']

        df['close_shifted'] = df['close'].shift(1).replace(0, epsilon)
        df['Open_to_LastClose'] = df['open'] / df['close_shifted']
        df['Close_to_LastClose'] = df['close'] / df['close_shifted']
        df['High_to_Close'] = df['high'] / df['close'].replace(0, epsilon)
        df['Low_to_Close'] = df['low'] / df['close'].replace(0, epsilon)

        df['volume_shifted'] = df['volume'].shift(1).replace(0, epsilon)
        df['Volume_to_LastVolume'] = df['volume'] / df['volume_shifted']

        # ë¬´í•œê°’ê³¼ ë„ˆë¬´ í° ê°’ ì œê±°
        df = df.replace([np.inf, -np.inf], np.nan)

        # NaN ê°’ ì œê±°
        df = df.dropna(subset=COLUMNS_TRAINING_DATA)
        print(f'After removing NaNs: {len(df)} rows')

        # ì •ê·œí™” ì ìš©
        if len(df) >= 100:  # ìµœì†Œ 100í–‰ í•„ìš” (ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœì†Œ ìš”êµ¬ì‚¬í•­)
            from sklearn.preprocessing import MinMaxScaler
            scaler = MinMaxScaler()
            numeric_columns = df[COLUMNS_TRAINING_DATA].columns
            df[numeric_columns] = scaler.fit_transform(df[numeric_columns])
            print(f'Features extracted: {len(df)} rows')
            return df
        else:
            print(f"Warning: Not enough valid rows after preprocessing ({len(df)}). Minimum 100 required.")
            return pd.DataFrame()

    except Exception as e:
        print(f'Error extracting features: {e}')
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

# ë©”ì¸ ì½”ë“œì—ì„œ filtered_results ë°ì´í„°í”„ë ˆì„ì— í•„ìš”í•œ ì—´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
if __name__ == '__main__':
    host = cf.MYSQL_HOST
    user = cf.MYSQL_USER
    password = cf.MYSQL_PASSWORD
    database_buy_list = cf.MYSQL_DATABASE_BUY_LIST
    database_craw = cf.MYSQL_DATABASE_CRAW
    results_table = cf.DENSE_UPDOWN_RESULTS_TABLE  # finding & training table
  # finding & training table
    performance_table = cf.LSTM_PERFORMANCE_TABLE  # ì„±ëŠ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  í…Œì´ë¸” ì´ë¦„
    # í…”ë ˆê·¸ë¨ ì„¤ì •
    telegram_token = cf.TELEGRAM_BOT_TOKEN
    telegram_chat_id = cf.TELEGRAM_CHAT_ID
    
    # í˜„ì¬ ë‚ ì§œ ì •ì˜
    current_date = datetime.now().strftime('%Y%m%d')
    
    # DBConnectionManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    buy_list_db = DBConnectionManager(host, user, password, database_buy_list)
    craw_db = DBConnectionManager(host, user, password, database_craw)
    
    # Load filtered stock results (ì°¾ì•„ë†“ì€ íŒ¨í„´ ê²°ê³¼)
    filtered_results = load_filtered_stock_results(buy_list_db, results_table)
    # workflow ì‹¤í–‰
    process_model_workflow(filtered_results, buy_list_db, craw_db, model_dir, results_table, current_date, telegram_token, telegram_chat_id)    
    # DB ì—°ê²° í•´ì œ
    buy_list_db.close()
    craw_db.close()