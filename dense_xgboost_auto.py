import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import f1_score, make_scorer
import os
import joblib
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
import cf
from mysql_loader import list_tables_in_database, load_data_from_mysql
from stock_utils import get_stock_items  # get_stock_items í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from tqdm import tqdm  # tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from telegram_utils import send_telegram_message  # í…”ë ˆê·¸ë¨ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from datetime import datetime, timedelta
from imblearn.over_sampling import SMOTE
from db_connection import DBConnectionManager


def execute_update_query(self, query):
    """
    INSERT, UPDATE, DELETE ì¿¼ë¦¬ì™€ ê°™ì€ ë°ì´í„° ìˆ˜ì • ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    try:
        with self.engine.connect() as conn:
            conn.execute(text(query))
            conn.commit()
        return True
    except Exception as e:
        print(f"Query execution error: {e}")
        return False

def load_filtered_stock_results(db_manager, table):
    try:
        query = f"SELECT * FROM {table}"
        df = db_manager.execute_query(query)
        return df
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        return pd.DataFrame()

def load_daily_craw_data(db_manager, table, start_date, end_date):
    try:
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')
        print(f"Loading data from {start_date_str} to {end_date_str} for table {table}")
        
        query = f"""
            SELECT * FROM `{table}`
            WHERE date >= '{start_date_str}' AND date <= '{end_date_str}'
            ORDER BY date ASC
        """
        
        df = db_manager.execute_query(query)
        print(f"Data loaded from {start_date_str} to {end_date_str} for table {table}: {len(df)} rows")
        return df
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        return pd.DataFrame()

def extract_features(df, COLUMNS_CHART_DATA):
    try:
        print(f'Original data rows: {len(df)}')
        print('Extracting features')

        # í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
        df = df[COLUMNS_CHART_DATA].copy()
        # ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
        df = df.sort_values(by='date')


        # ì´ë™í‰ê·  ê³„ì‚°
        df['MA5'] = df['close'].rolling(window=5).mean()
        df['MA10'] = df['close'].rolling(window=10).mean()
        df['MA20'] = df['close'].rolling(window=20).mean()
        df['MA60'] = df['close'].rolling(window=60).mean()
        df['MA120'] = df['close'].rolling(window=120).mean()
        df['MA240'] = df['close'].rolling(window=240).mean()
        
        # ì´ë™í‰ê· ê³¼ ì¢…ê°€ì˜ ë¹„ìœ¨ ê³„ì‚°
        df['Close_to_MA5'] = df['close'] / df['MA5']
        df['Close_to_MA10'] = df['close'] / df['MA10']
        df['Close_to_MA20'] = df['close'] / df['MA20']
        df['Close_to_MA60'] = df['close'] / df['MA60']
        df['Close_to_MA120'] = df['close'] / df['MA120']
        df['Close_to_MA240'] = df['close'] / df['MA240']
        
        # ê±°ë˜ëŸ‰ ì´ë™í‰ê·  ê³„ì‚°
        df['Volume_MA5'] = df['volume'].rolling(window=5).mean()
        df['Volume_MA10'] = df['volume'].rolling(window=10).mean()
        df['Volume_MA20'] = df['volume'].rolling(window=20).mean()
        df['Volume_MA60'] = df['volume'].rolling(window=60).mean()
        df['Volume_MA120'] = df['volume'].rolling(window=120).mean()
        df['Volume_MA240'] = df['volume'].rolling(window=240).mean()
        
        # ê±°ë˜ëŸ‰ê³¼ ì´ë™í‰ê· ì˜ ë¹„ìœ¨ ê³„ì‚°
        df['Volume_to_MA5'] = df['volume'] / df['Volume_MA5']
        df['Volume_to_MA10'] = df['volume'] / df['Volume_MA10']
        df['Volume_to_MA20'] = df['volume'] / df['Volume_MA20']
        df['Volume_to_MA60'] = df['volume'] / df['Volume_MA60']
        df['Volume_to_MA120'] = df['volume'] / df['Volume_MA120']
        df['Volume_to_MA240'] = df['volume'] / df['Volume_MA240']
        
        # ì¶”ê°€ ë¹„ìœ¨ ê³„ì‚°
        df['Open_to_LastClose'] = df['open'] / df['close'].shift(1)
        df['Close_to_LastClose'] = df['close'] / df['close'].shift(1)
        df['High_to_Close'] = df['high'] / df['close']
        df['Low_to_Close'] = df['low'] / df['close']
        
        df['Volume_to_LastVolume'] = df['volume'] / df['volume'].shift(1)
        
        df = df.dropna()
        print(f'Features extracted: {len(df)}')
        # print(df.head())
        # print(df.tail())
        return df
    except Exception as e:
        print(f'Error extracting features: {e}')
        return pd.DataFrame()

def label_data(df, signal_dates):
    try:
        print('Labeling data')
        
        df['Label'] = 0  # ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
        df['date'] = pd.to_datetime(df['date']).dt.date  # ë‚ ì§œ í˜•ì‹ì„ datetime.dateë¡œ ë³€í™˜
        
        # signal_datesë¥¼ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì˜ëª»ëœ í˜•ì‹ì˜ ë‚ ì§œë¥¼ ì²˜ë¦¬
        valid_signal_dates = []
        for date in signal_dates:
            try:
                valid_date = pd.to_datetime(date).date()
                valid_signal_dates.append(valid_date)
            except ValueError:
                print(f"Invalid date format: {date}")
        
        # ë‚ ì§œ ì •ë ¬
        valid_signal_dates.sort()
        print(f'Signal dates: {valid_signal_dates}')
        
        if len(valid_signal_dates) > 0:
            # 3ê°œì›”(ì•½ 90ì¼) ì´ìƒ ì°¨ì´ë‚˜ëŠ” ë‚ ì§œë¡œ ê·¸ë£¹ ë¶„í• 
            date_groups = []
            current_group = [valid_signal_dates[0]]
            
            for i in range(1, len(valid_signal_dates)):
                days_diff = (valid_signal_dates[i] - valid_signal_dates[i-1]).days
                if days_diff >= 90:  # 3ê°œì›” ì´ìƒ ì°¨ì´
                    date_groups.append(current_group)
                    current_group = [valid_signal_dates[i]]
                else:
                    current_group.append(valid_signal_dates[i])
            
            date_groups.append(current_group)
            
            print(f"Found {len(date_groups)} separate signal groups")
            
            # ê° ê·¸ë£¹ ì²˜ë¦¬
            for group_idx, group in enumerate(date_groups):
                print(f"Processing group {group_idx+1} with {len(group)} signals")
                
                # ê·¸ë£¹ì˜ ì‹œì‘ê³¼ ë ë‚ ì§œ
                start_date = min(group)
                end_date = max(group)
                
                # ì›ë˜ ì‹ í˜¸ ë‚ ì§œë“¤ì„ 3ë“±ë¶„í•˜ì—¬ ë¼ë²¨ ë¶€ì—¬
                n = len(group)
                first_third = group[:n//3] if n > 2 else group
                second_third = group[n//3:2*n//3] if n > 2 else []
                last_third = group[2*n//3:] if n > 2 else []
                
                # ì›ë³¸ ì‹ í˜¸ ë‚ ì§œì— ë¼ë²¨(1,2,3) ë¶€ì—¬
                signal_labels = {}
                for date in first_third:
                    signal_labels[date] = 1
                for date in second_third:
                    signal_labels[date] = 2
                for date in last_third:
                    signal_labels[date] = 3
                
                # ê° ì‹ í˜¸ ë‚ ì§œë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì ìš©
                sorted_dates = df[(df['date'] >= start_date) & (df['date'] <= end_date)]['date'].unique()
                sorted_dates.sort()
                
                # ê° ë‚ ì§œì— ëŒ€í•´ ì²˜ë¦¬
                current_label = 0
                for date in sorted_dates:
                    if date in signal_labels:
                        # ì‹ í˜¸ ë‚ ì§œì¸ ê²½ìš° í•´ë‹¹ ë¼ë²¨ë¡œ ì„¤ì •
                        current_label = signal_labels[date]
                    
                    # í˜„ì¬ ë¼ë²¨(ì´ì „ ì‹ í˜¸ì™€ ê°™ì€ ë¼ë²¨)ì„ ì ìš©
                    df.loc[df['date'] == date, 'Label'] = current_label
        
        print(f'Data labeled: {len(df)} rows')

        # ë¼ë²¨ ë¶„í¬ ì¶œë ¥
        print("Label distribution:")
        print(df['Label'].value_counts())
        
        # ì²« 5ê°œì™€ ë§ˆì§€ë§‰ 10ê°œì˜ ë¼ë²¨ ì¶œë ¥
        print("First 5 labels:")
        print(df[['date', 'Label']].head(3))
        print("Last 10 labels:")
        print(df[['date', 'Label']].tail(15))

        return df
    except Exception as e:
        print(f'Error labeling data: {e}')
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ traceback ì •ë³´ ì¶œë ¥
        return pd.DataFrame()

def train_model(X, y, use_saved_params=True, param_file='best_params.pkl'):
    try:
        print('Training model')
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        y = y[X.index]
        
        print("Class distribution in y:")
        print(y.value_counts())
        
        # Calculate class weights
        class_weights = {0: 1, 1: 1, 2: 1, 3: 1}  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
        for class_label, weight in y.value_counts(normalize=True).items():
            class_weights[class_label] = weight
        
        sample_weights = np.array([1/class_weights[yi] for yi in y])
        
        print(f"use_saved_params: {use_saved_params}")  # use_saved_params ê°’ ì¶œë ¥
        print(f"param_file exists: {os.path.exists(param_file)}")  # param_file ì¡´ì¬ ì—¬ë¶€ ì¶œë ¥
        
        if use_saved_params and os.path.exists(param_file):
            print("Loading saved parameters...")
            try:
                best_params = joblib.load(param_file)
                model = xgb.XGBClassifier(
                    **best_params,
                    random_state=42,
                    objective='multi:softmax',
                    num_class=4,
                    eval_metric='mlogloss'
                )
                print("Model loaded with saved parameters.")
            except Exception as e:
                print(f"Error loading saved parameters: {e}")
                model = None  # ë¡œë”© ì‹¤íŒ¨ ì‹œ modelì„ Noneìœ¼ë¡œ ì„¤ì •
        else:
            print("Tuning hyperparameters...")
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 1.0],
                'colsample_bytree': [0.8, 1.0],
                'min_child_weight': [1, 3, 5]
            }
            
            base_model = xgb.XGBClassifier(
                random_state=42,
                objective='multi:softmax',
                num_class=4,
                eval_metric='mlogloss'
            )
            
            grid_search = GridSearchCV(
                estimator=base_model,
                param_grid=param_grid,
                cv=3,
                scoring='f1_weighted',
                n_jobs=-1,
                verbose=2
            )
            
            # Train with sample weights
            grid_search.fit(X, y, sample_weight=sample_weights)
            
            best_params = grid_search.best_params_
            print(f'Best parameters found: {best_params}')
            joblib.dump(best_params, param_file)
            model = grid_search.best_estimator_
            print("Model trained with new parameters.")
        
        if model is not None:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Calculate weights for training set
            train_class_weights = {0: 1, 1: 1, 2: 1, 3: 1}  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
            for class_label, weight in y_train.value_counts(normalize=True).items():
                train_class_weights[class_label] = weight
            
            train_weights = np.array([1/train_class_weights[yi] for yi in y_train])
            
            # Train final model with weights
            model.fit(X_train, y_train, sample_weight=train_weights)
            
            y_pred = model.predict(X_test)
            print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
            print('Classification Report:')
            print(classification_report(y_test, y_pred))
        else:
            print("Model training failed.")
        
        return model
    except Exception as e:
        print(f'Error training model: {e}')
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ traceback ì •ë³´ ì¶œë ¥
        return None


def predict_pattern(model, df, stock_name, use_data_dates=True, settings=None):
    # í•¨ìˆ˜ ë‚´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„¤ì •ì€ ì§€ì—­ ë³€ìˆ˜ë¡œ ì¶”ì¶œ
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    try:
        print('Predicting patterns')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'stock_name'])
        X = df[COLUMNS_TRAINING_DATA]  # ì§€ì—­ ë³€ìˆ˜ë¡œ ê°„ê²°í•˜ê²Œ ì‚¬ìš©
     
        # ë¬´í•œëŒ€ ê°’ì´ë‚˜ ë„ˆë¬´ í° ê°’ ì œê±°
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        predictions = model.predict(X)
        df = df.loc[X.index]  # ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€
        df['Prediction'] = predictions
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > 0).sum()} matches found')
        
        # ë‚ ì§œ í˜•ì‹ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        try:
            # MySQLì˜ YYYYMMDD í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            if df['date'].dtype == 'object':
                # YYYYMMDD í˜•ì‹ì˜ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # NaT ê°’ ì œê±°
            df = df.dropna(subset=['date'])
            print(f"Date range in data: {df['date'].min()} to {df['date'].max()}")
            
            # ê²€ì¦ ê¸°ê°„ ì„¤ì • ë¶€ë¶„ ìˆ˜ì •
            if use_data_dates:
                # ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œ ì´í›„ë¡œ ì˜ˆì¸¡ ê²€ì¦ ê¸°ê°„ ì„¤ì • (í›ˆë ¨ ì§í›„ ê²€ì¦ìš©)
                max_date = df['date'].max()
                validation_start_date = max_date + pd.Timedelta(days=1)
                validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
            else:
                # cf.pyì— ì„¤ì •ëœ ê²€ì¦ ê¸°ê°„ ì‚¬ìš© (ì™¸ë¶€ ê²€ì¦ìš©)
                validation_start_date = pd.to_datetime(str(cf.VALIDATION_START_DATE_AUTO).zfill(8), format='%Y%m%d')
                validation_end_date = pd.to_datetime(str(cf.VALIDATION_END_DATE_AUTO).zfill(8), format='%Y%m%d')
            
            print(f"Validation period: {validation_start_date} to {validation_end_date}")
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ íŒ¨í„´ í•„í„°ë§ (Predictionì´ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ)
            recent_patterns = df[
                (df['Prediction'] > 0) & 
                (df['date'] >= validation_start_date) & 
                (df['date'] <= validation_end_date)
            ].copy()
            
            print(f'Filtered patterns in validation period: {len(recent_patterns)}')
            
            if not recent_patterns.empty:
                recent_patterns['stock_name'] = stock_name
                result = recent_patterns[['date', 'stock_name']]
                print(f'Found patterns for {stock_name}:')
                print(result)
                return result
            else:
                print(f'No patterns found for {stock_name} in validation period')
                return pd.DataFrame(columns=['date', 'stock_name'])
                
        except Exception as e:
            print(f"Error in date processing: {e}")
            print(f"Debug info - df['date'] sample: {df['date'].head()}")
            print(f"Debug info - validation dates: {cf.VALIDATION_END_DATE_AUTO}")
            return pd.DataFrame(columns=['date', 'stock_name'])
            
    except Exception as e:
        print(f'Error predicting patterns: {e}')
        print(f'Error type: {type(e).__name__}')
        import traceback
        print(f'Stack trace:\n{traceback.format_exc()}')
        return pd.DataFrame(columns=['date', 'stock_name'])


def evaluate_performance(df, start_date, end_date):
    try:
        print('Evaluating performance')
        df['date'] = pd.to_datetime(df['date'])
        
        # ë‹¤ìŒë‚  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(ì˜¤ëŠ˜ì´ ë§ˆì§€ë§‰ ë‚ ì§œì¸ ê²½ìš°) ì²´í¬
        if df[df['date'] >= start_date].empty:
            print(f"No data available from {start_date} (next trading day). Returning 0.")
            return 0.0
        
        # ë§¤ìˆ˜ì¼(start_date)ì˜ ì¢…ê°€ ê°€ì ¸ì˜¤ê¸° - ë§¤ìˆ˜ê°€ê²© ì„¤ì •
        buy_data = df[df['date'] >= start_date].iloc[0]
        buy_price = buy_data['close']
        buy_date = buy_data['date']
        
        # ë§¤ìˆ˜ì¼ë¶€í„° 60ì¼ê°„ì˜ ë°ì´í„° ì„ íƒ
        period_df = df[(df['date'] >= buy_date) & (df['date'] <= end_date)]
        
        if period_df.empty or len(period_df) < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”
            print(f"Insufficient data between {buy_date} and {end_date}")
            return 0.0
        
        # ìµœëŒ€ ìˆ˜ìµë¥  ê³„ì‚° (ìµœê³ ê°€ ê¸°ì¤€)
        max_price = period_df['high'].max()
        max_profit_rate = (max_price - buy_price) / buy_price * 100
        
        # ìµœëŒ€ ì†ì‹¤ë¥  ê³„ì‚° (ìµœì €ê°€ ê¸°ì¤€)
        min_price = period_df['low'].min()
        max_loss_rate = (min_price - buy_price) / buy_price * 100  # ì†ì‹¤ì€ ìŒìˆ˜ë¡œ í‘œí˜„ë¨
        
        # ì˜ˆìƒ ìˆ˜ìµë¥  = ìµœëŒ€ ìˆ˜ìµë¥  - |ìµœëŒ€ ì†ì‹¤ë¥ |
        estimated_profit_rate = max_profit_rate - abs(max_loss_rate)
        
        print(f"Buy price: {buy_price}, Max price: {max_price}, Min price: {min_price}")
        print(f"Max profit: {max_profit_rate:.2f}%, Max loss: {max_loss_rate:.2f}%, Estimated profit: {estimated_profit_rate:.2f}%")
        
        return estimated_profit_rate
        
    except Exception as e:
        print(f'Error evaluating performance: {e}')
        import traceback
        traceback.print_exc()
        return 0.0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0 ë°˜í™˜

def save_xgboost_to_deep_learning_table(performance_df, buy_list_db, model_name='xgboost'):
    """ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤. ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í•­ëª©ì€ ê±´ë„ˆëœë‹ˆë‹¤."""
    try:
        # ìƒˆë¡œìš´ ë°ì´í„° êµ¬ì„±
        deep_learning_data = []
        
        for _, row in performance_df.iterrows():
            deep_learning_data.append({
                'date': row['pattern_date'],
                'method': 'xgboost_weighted',  # í•˜ë“œì½”ë”©ëœ 'xgboost' ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ model_name ì‚¬ìš©
                'stock_name': row['stock_name'],
                'confidence': row.get('confidence', 0),  # ì‹ ë¢°ë„ê°’ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0
                'estimated_profit_rate': row['max_return']
            })
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        deep_learning_df = pd.DataFrame(deep_learning_data)
        
        if deep_learning_df.empty:
            print("No data to save to deep_learning table")
            return False
        
        # ì €ì¥í•˜ë ¤ëŠ” ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        start_date = deep_learning_df['date'].min()
        end_date = deep_learning_df['date'].max()
        
        # ê¸°ì¡´ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„° ì¡°íšŒ
        existing_query = f"""
            SELECT date, method, stock_name FROM deep_learning 
            WHERE date >= '{start_date}' AND date <= '{end_date}' 
            AND method = '{model_name}'
        """
        existing_data = buy_list_db.execute_query(existing_query)
        
        # ê¸°ì¡´ í•­ëª©ì˜ ê³ ìœ  ì‹ë³„ì ì„¸íŠ¸ ìƒì„± (date, method, stock_name ì¡°í•©)
        existing_items = set()
        for _, row in existing_data.iterrows():
            # ë‚ ì§œ í˜•ì‹ í†µì¼ (ë¬¸ìì—´ë¡œ ë³€í™˜)
            date_str = row['date'].strftime('%Y-%m-%d') if isinstance(row['date'], pd.Timestamp) else str(row['date'])
            existing_items.add((date_str, row['method'], row['stock_name']))
        
        # ìƒˆë¡œ ì¶”ê°€í•  í•­ëª©ë§Œ í•„í„°ë§
        new_items = []
        for _, row in deep_learning_df.iterrows():
            date_str = row['date'].strftime('%Y-%m-%d') if isinstance(row['date'], pd.Timestamp) else str(row['date'])
            if (date_str, row['method'], row['stock_name']) not in existing_items:
                new_items.append(row)
        
        # ìƒˆ í•­ëª©ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not new_items:
            print("All items already exist in deep_learning table. No new data to insert.")
            return True
        
        # ìƒˆ í•­ëª©ë§Œ ì‚½ì…
        print(f"Adding {len(new_items)} new items to deep_learning table")
        for row in new_items:
            # ë‚ ì§œê°€ datetime ê°ì²´ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            date_str = row['date'].strftime('%Y-%m-%d') if isinstance(row['date'], pd.Timestamp) else row['date']
            insert_query = f"""
                INSERT INTO deep_learning (date, method, stock_name, confidence, estimated_profit_rate)
                VALUES ('{date_str}', '{row['method']}', '{row['stock_name']}', {row['confidence']}, {row['estimated_profit_rate']})
            """
            buy_list_db.execute_update_query(insert_query)
        
        print(f"{model_name} ì„±ëŠ¥ ê²°ê³¼ê°€ deep_learning í…Œì´ë¸”ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒˆë¡œ ì¶”ê°€ëœ í•­ëª©: {len(new_items)}ê°œ)")
        return True
    except Exception as e:
        print(f"deep_learning í…Œì´ë¸” ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


def evaluate_model_performance(validation_results, buy_list_db, craw_db, settings, model_filename=None):
    """ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    try:
        if validation_results.empty:
            print("No validation results to evaluate.")
            return pd.DataFrame()
        
        print(f"\nEvaluating performance for {len(validation_results)} validation results...")
        
        # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        performance_results = []
        
        for index, row in tqdm(validation_results.iterrows(), total=len(validation_results), desc="Evaluating performance"):
            stock_name = row['stock_name']
            pattern_date = row['date']
            prediction_score = row['score']  # ì˜ˆì¸¡ ì ìˆ˜ ì €ì¥
            
            # ì¼ì • ê¸°ê°„ ë™ì•ˆì˜ ì„±ëŠ¥ ì¸¡ì •
            performance_start_date = pattern_date + pd.Timedelta(days=1)  # íŒ¨í„´ ë‹¤ìŒ ë‚ ë¶€í„°
            performance_end_date = pattern_date + pd.Timedelta(days=60)   # 60ì¼ ë™ì•ˆ
            
            # ë°ì´í„° ë¡œë“œ
            df = load_daily_craw_data(craw_db, stock_name, performance_start_date, performance_end_date)
            print(f"Evaluating performance for {stock_name} from {performance_start_date} to {performance_end_date}: {len(df)} rows")
            
            # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ë³€í™˜
            if not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
                # ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ ì œê±°
                df = df.dropna(subset=['date'])
                print(f"Converted date column to datetime. Remaining rows: {len(df)}")
            
            # ì¤‘ìš” ë³€ê²½: ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë‹¤ìŒ ë‚  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ê²°ê³¼ì— í¬í•¨
            is_latest_data = False
            max_return = 0.0
            
            if df.empty:
                print(f"No data available for {stock_name} after {performance_start_date}. This might be the latest pattern.")
                is_latest_data = True
            else:
                # ë‹¤ìŒë‚  ë°ì´í„° ìœ ë¬´ í™•ì¸
                if df[df['date'] >= performance_start_date].empty:
                    print(f"This is the latest data available for {stock_name}. Next trading day not available yet.")
                    is_latest_data = True
                else:
                    # ì„±ëŠ¥ ê³„ì‚°
                    max_return = evaluate_performance(df, performance_start_date, performance_end_date)
            
            # ëª¨ë“  ì¼€ì´ìŠ¤ì— ëŒ€í•´ ê²°ê³¼ ì €ì¥ (ìµœì‹  ë°ì´í„° ì—¬ë¶€ í‘œì‹œ í¬í•¨)
            performance_results.append({
                'stock_name': stock_name,
                'pattern_date': pattern_date,
                'start_date': performance_start_date,
                'end_date': performance_end_date,
                'max_return': round(max_return, 2),  # ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€
                'prediction_score': round(prediction_score, 4),  # ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€
                'confidence': round(prediction_score, 4),  # ê¸°ë³¸ ì‹ ë¢°ë„ ê°’
                'is_latest': is_latest_data  # ìµœì‹  ë°ì´í„° ì—¬ë¶€ í‘œì‹œ (ì¶”ê°€ í•„ë“œ)
            })
        
        # ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        performance_df = pd.DataFrame(performance_results)
        
        if not performance_df.empty:
            # ìµœì‹  ë°ì´í„°ì™€ íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
            latest_results = performance_df[performance_df['is_latest'] == True]
            history_results = performance_df[performance_df['is_latest'] == False]
            
            print("\nLatest patterns (no performance data yet):")
            if not latest_results.empty:
                print(latest_results[['stock_name', 'pattern_date']])
            else:
                print("None")
                
            print("\nHistorical performance results:")
            if not history_results.empty:
                print(history_results[['stock_name', 'pattern_date', 'prediction_score', 'max_return']])
                
                # íˆìŠ¤í† ë¦¬ ê²°ê³¼ì— ëŒ€í•œ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚°
                avg_return = history_results['max_return'].mean()
                max_profit = history_results['max_return'].max()
                min_profit = history_results['max_return'].min()
                
                # ìƒê´€ ê³„ìˆ˜ ê³„ì‚°
                if 'prediction_score' in history_results.columns:
                    corr = history_results['prediction_score'].corr(history_results['max_return'])
                    print(f"\nì˜ˆì¸¡ ì ìˆ˜ì™€ ì‹¤ì œ ìˆ˜ìµë¥ ì˜ ìƒê´€ê³„ìˆ˜: {corr:.4f}")
                
                print(f"\nAverage historical return: {avg_return:.2f}%")
                print(f"Maximum historical return: {max_profit:.2f}%")
                print(f"Minimum historical return: {min_profit:.2f}%")
            
            # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            # save_performance_to_db(performance_df, buy_list_db, settings['performance_table'])
            
            # deep_learning í…Œì´ë¸”ì—ë„ ì €ì¥
            save_xgboost_to_deep_learning_table(performance_df, buy_list_db, 'dense_xgboost')
            
            # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
            telegram_token = settings['telegram_token']
            telegram_chat_id = settings['telegram_chat_id']
            
            # ë©”ì‹œì§€ ì´ˆê¸°í™”
            message = "XGBoost performance results:\n\n"
            
            # íˆìŠ¤í† ë¦¬ ê²°ê³¼ í‘œì‹œ
            if not history_results.empty:
                message += "ğŸ“ˆ HISTORICAL PERFORMANCE:\n"
                # ìˆ˜ìµë¥  ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_history = history_results.sort_values(by='pattern_date', ascending=True)
                for _, row in sorted_history.iterrows():
                    message += f"{row['pattern_date'].strftime('%Y-%m-%d')}: {row['stock_name']} - Score: {row['prediction_score']:.4f}, Return: {row['max_return']:.2f}%\n"
                
                message += f"\nAverage return: {avg_return:.2f}%"
                if 'prediction_score' in history_results.columns:
                    message += f"\nì˜ˆì¸¡ ì ìˆ˜ì™€ ìˆ˜ìµë¥ ì˜ ìƒê´€ê³„ìˆ˜: {corr:.4f}"

            # ìµœì‹  íŒ¨í„´ ë‚˜ì¤‘ì— í‘œì‹œ
            if not latest_results.empty:
                message += "ğŸ“Š LATEST PATTERNS (Today's signals):\n"
                for _, row in latest_results.iterrows():
                    message += f"ğŸ” {row['pattern_date'].strftime('%Y-%m-%d')}: {row['stock_name']} - Score: {row['prediction_score']:.4f}\n"
                message += "\n"

            send_telegram_message(telegram_token, telegram_chat_id, message)
            
            return performance_df
        else:
            print("No performance results generated.")
            return pd.DataFrame()
        
    except Exception as e:
        print(f"Error evaluating model performance: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def save_performance_to_db(df, db_manager, table):
    try:
        result = db_manager.to_sql(df, table)
        if result:
            print(f"Performance results saved to {table} table in {db_manager.database} database")
        return result
    except Exception as e:
        print(f"Error saving performance results to MySQL: {e}")
        return False


def setup_environment():
    """í™˜ê²½ ì„¤ì • ë° í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("Starting pattern recognition by xgboost...")
    
    # ê¸°ë³¸ ì„¤ì •
    host = cf.MYSQL_HOST
    user = cf.MYSQL_USER
    password = cf.MYSQL_PASSWORD
    database_buy_list = cf.MYSQL_DATABASE_BUY_LIST
    database_craw = cf.MYSQL_DATABASE_CRAW
    results_table = cf.FINDING_SKYROCKET_TABLE
    performance_table = cf.RECOGNITION_PERFORMANCE_TABLE
    telegram_token = cf.TELEGRAM_BOT_TOKEN
    telegram_chat_id = cf.TELEGRAM_CHAT_ID
    
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    model_dir = 'models'
    os.makedirs(model_dir, exist_ok=True)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì ìƒì„±
    buy_list_db = DBConnectionManager(host, user, password, database_buy_list)
    craw_db = DBConnectionManager(host, user, password, database_craw)

    # ì—´ ì •ì˜
    COLUMNS_CHART_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']
    COLUMNS_TRAINING_DATA = [
        'Open_to_LastClose', 'High_to_Close', 'Low_to_Close',
        'Close_to_LastClose', 'Volume_to_LastVolume',
        'Close_to_MA5', 'Volume_to_MA5',
        'Close_to_MA10', 'Volume_to_MA10',
        'Close_to_MA20', 'Volume_to_MA20',
        'Close_to_MA60', 'Volume_to_MA60',
        'Close_to_MA120', 'Volume_to_MA120',
        'Close_to_MA240', 'Volume_to_MA240',
    ]
    
    # ì„¤ì • ì‚¬ì „ ìƒì„±
    settings = {
        'host': host,
        'user': user,
        'password': password,
        'database_buy_list': database_buy_list,
        'database_craw': database_craw,
        'results_table': results_table,
        'performance_table': performance_table,
        'telegram_token': telegram_token,
        'telegram_chat_id': telegram_chat_id,   
        'COLUMNS_CHART_DATA': COLUMNS_CHART_DATA,
        'COLUMNS_TRAINING_DATA': COLUMNS_TRAINING_DATA,
        'model_dir': model_dir,
        'current_date': datetime.now().strftime('%Y%m%d'),
        'param_file': 'best_params.pkl'
    }
    
    return buy_list_db, craw_db, settings

# def load_or_train_model(buy_list_db, craw_db, filtered_results, settings):
#     """ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
#     model_dir = settings['model_dir']
#     results_table = settings['results_table']
#     current_date = settings['current_date']
#     telegram_token = settings['telegram_token']
#     telegram_chat_id = settings['telegram_chat_id']
    
#     model_filename = os.path.join(model_dir, f"{results_table}_{current_date}.json")
#     print(f"Model filename: {model_filename}")
    
#     # ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ í›ˆë ¨ ì—¬ë¶€ ì§ˆë¬¸ (ê¸°ë³¸ê°’: 'no')
#     # choice = input("Do you want to retrain the model? (yes/no) [no]: ").strip().lower()
#     choice = 'no'  # ì„ì‹œë¡œ ê¸°ë³¸ê°’ì„ 'no'ë¡œ ì„¤ì •
#     if not choice:  # ì…ë ¥ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
#         choice = 'no'
    
#     print(f"User choice: {choice}")
    
#     if choice == 'yes':
#         # ì‚¬ìš©ìê°€ 'ì˜ˆ'ë¥¼ ì„ íƒí•œ ê²½ìš° - ëª¨ë¸ ì¬í›ˆë ¨
#         print("User selected to retrain the model.")
#         print("Will proceed to train_models function...")
#         return None, 0.0, True  # ëª¨ë¸ ì—†ìŒ, ì •í™•ë„ 0, retrain=True
#     elif choice == 'no':
#         model_filename = os.path.join(model_dir, f"dense_results_2013_best.json")
#         print(f"Model filename: {model_filename}")
    
#         # ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
#         available_models = [f for f in os.listdir(model_dir) if f.endswith('.json')]
        
#         if not available_models:
#             print("No saved models found. Will train a new model.")
#             return None, 0.0, True
#         else:
#             print("\nAvailable models:")
#             for i, model_file in enumerate(available_models):
#                 print(f"{i+1}. {model_file}")
            
#             # ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ ì„ íƒ ìš”ì²­
#             while True:
#                 try:
#                     model_choice = input("\nSelect a model number (or type 'new' to train a new model): ")
                    
#                     if model_choice.lower() == 'new':
#                         print("User selected to train a new model.")
#                         return None, 0.0, True
#                     else:
#                         model_index = int(model_choice) - 1
#                         if 0 <= model_index < len(available_models):
#                             model_filename = os.path.join(model_dir, available_models[model_index])
#                             print(f"Loading model: {model_filename}")
#                             model = xgb.XGBClassifier()
#                             model.load_model(model_filename)
#                             return model, 0.0, False  # ë¡œë“œí•œ ëª¨ë¸, ì •í™•ë„, retrain ì—¬ë¶€
#                         else:
#                             print("Invalid model number. Please try again.")
#                 except ValueError:
#                     print("Invalid input. Please enter a number or 'new'.")
#     else:
#         # ì˜ëª»ëœ ì…ë ¥ì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ 'no' ì²˜ë¦¬
#         print(f"Invalid choice: '{choice}'. Defaulting to 'no'.")
#         return load_or_train_model(buy_list_db, craw_db, filtered_results, settings)  # ì¬ê·€ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸

def load_or_train_model(buy_list_db, craw_db, filtered_results, settings):
    """ì§€ì •ëœ ëª¨ë¸ë¡œ ë°”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤."""
    model_dir = settings['model_dir']
    results_table = settings['results_table']
    current_date = settings['current_date']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    
    # ì§€ì •í•  ëª¨ë¸ íŒŒì¼ëª…
    specified_model = "dense_results_2013_best.json"
    
    model_path = os.path.join(model_dir, specified_model)
    if os.path.exists(model_path):
        print(f"Loading specified model: {model_path}")
        model = xgb.XGBClassifier()
        model.load_model(model_path)
        return model, 0.0, False  # ëª¨ë¸ ë¡œë“œ ì„±ê³µ, retrain=False
    else:
        print(f"âš ï¸ ì§€ì •ëœ ëª¨ë¸ '{specified_model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return None, 0.0, True  # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, retrain=True

def train_models(buy_list_db, craw_db, filtered_results, settings):
    """XGBoost ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    print("Retraining the model...")
    param_file = settings['param_file']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    # ì²« ë²ˆì§¸ ì¢…ëª©ì— ëŒ€í•´ì„œë§Œ use_saved_paramsë¥¼ Falseë¡œ ì„¤ì •
    first_stock = True
    best_model = None
    best_accuracy = 0
    total_models = 0
    successful_models = 0
    
    # ì¢…ëª©ë³„ë¡œ ê·¸ë£¹í™”
    grouped_results = filtered_results.groupby('stock_name')
    
    # ê° ê·¸ë£¹ì˜ ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ë©° ì¢…ëª©ë³„, ê·¸ë£¹ë³„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëª¨ë¸ì„ í›ˆë ¨
    for stock_name, group in tqdm(grouped_results, desc="Training models"):
        signal_dates = group['signal_date'].tolist()
        
        # ë¬¸ìì—´ í˜•íƒœì˜ signal_datesë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        valid_signal_dates = []
        for date in signal_dates:
            if isinstance(date, str):
                try:
                    valid_date = pd.to_datetime(date.strip(), format='%Y%m%d').date()
                    valid_signal_dates.append(valid_date)
                except ValueError:
                    print(f"Invalid date format: {date}")
            else:
                print(f"Invalid date type: {date}")
    
        if not valid_signal_dates:
            print(f"No valid signal dates for {stock_name}")
            continue
        
        # 3ê°œì›”(ì•½ 90ì¼) ì´ìƒ ì°¨ì´ë‚˜ëŠ” ë‚ ì§œë¡œ ê·¸ë£¹ ë¶„í• 
        date_groups = []
        current_group = [valid_signal_dates[0]]
        
        for i in range(1, len(valid_signal_dates)):
            days_diff = (valid_signal_dates[i] - valid_signal_dates[i-1]).days
            if days_diff >= 90:  # 3ê°œì›” ì´ìƒ ì°¨ì´
                date_groups.append(current_group)
                current_group = [valid_signal_dates[i]]
            else:
                current_group.append(valid_signal_dates[i])
        
        date_groups.append(current_group)
        
        # ê° ê·¸ë£¹ë³„ë¡œ ë³„ë„ ëª¨ë¸ í›ˆë ¨
        for group_idx, signal_group in enumerate(date_groups):
            end_date = max(signal_group)  # ê·¸ë£¹ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ
            start_date = end_date - timedelta(days=1200)
            
            print(f"\nTraining model for {stock_name} - Group {group_idx+1}: {start_date} to {end_date}")
            df = load_daily_craw_data(craw_db, stock_name, start_date, end_date)
            
            if df.empty:
                continue
                
            # íŠ¹ì„± ì¶”ì¶œ ë° ë¼ë²¨ë§
            df = extract_features(df, settings['COLUMNS_CHART_DATA'])
            df = label_data(df, signal_group)  # í•´ë‹¹ ê·¸ë£¹ì˜ ë‚ ì§œë§Œ ì „ë‹¬
            # 500ë´‰ë§Œ ì˜ë¼ì„œ í›ˆë ¨
            if len(df) > 500:
                df = df[-500:]
                
            # ëª¨ë¸ í›ˆë ¨
            X = df[COLUMNS_TRAINING_DATA]
            y = df['Label']
            model = train_model(X, y, use_saved_params=(not first_stock), param_file=param_file)
            
            # ëª¨ë¸ í‰ê°€ ë° ì €ì¥
            if model:
                # í›ˆë ¨ ì •ë³´ ì¶œë ¥
                print(f"Model trained for {stock_name} from {start_date} to {end_date}")
                
                # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•˜ê¸° ìœ„í•´ ì„±ëŠ¥ í‰ê°€
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                if accuracy > best_accuracy or best_model is None:
                    best_model = model
                    best_accuracy = accuracy
                    print(f"New best model found for {stock_name} with accuracy: {accuracy:.4f}")
            else:
                print(f"Model training failed for {stock_name}")
        
            total_models += 1
            if model:
                successful_models += 1
        
        # ì²« ë²ˆì§¸ ì¢…ëª© ì²˜ë¦¬ í›„ í”Œë˜ê·¸ ë³€ê²½
        first_stock = False
    
    print(f"\nTotal models trained: {total_models}")
    print(f"Successful models: {successful_models}")
    
    # í›ˆë ¨ì´ ëë‚œ í›„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë³´ë‚´ê¸°
    message = f"Training completed.\nTotal models trained: {total_models}\nSuccessful models: {successful_models}"
    send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return best_model, best_accuracy


def save_model(model, accuracy, settings):
    """í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    model_dir = settings['model_dir']
    results_table = settings['results_table']
    current_date = settings['current_date']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    
    model_filename = os.path.join(model_dir, f"{results_table}_{current_date}.json")
    
    if model:
        print("Saving best model...")
        model.save_model(model_filename)
        print(f"Best model saved as {model_filename} with accuracy: {accuracy:.4f}")
        message = f"Best model saved as {model_filename} with accuracy: {accuracy:.4f}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    else:
        print("No model to save.")
        pause = input("Press Enter to continue...")
        message = "No model to save."
        send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return model_filename  # íŒŒì¼ ì´ë¦„ ë°˜í™˜


def validate_model(model, buy_list_db, craw_db, settings):
    """í•™ìŠµëœ ëª¨ë¸ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    results_table = settings['results_table']
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    # ê²€ì¦ ê¸°ê°„ ì„¤ì •
    print(f"\nLoading data for validation from {cf.VALIDATION_START_DATE_AUTO} to {cf.VALIDATION_END_DATE_AUTO}")
    validation_start_date = pd.to_datetime(cf.VALIDATION_START_DATE_AUTO)
    validation_end_date = pd.to_datetime(cf.VALIDATION_END_DATE_AUTO)
    
    # ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ê²€ì¦ ë°ì´í„° ë¡œë“œ
    stock_items = get_stock_items(settings['host'], settings['user'], settings['password'], settings['database_buy_list'])
    total_stock_items = len(stock_items)
    print(f"\nì „ì²´ ì¢…ëª© ìˆ˜: {total_stock_items}")
    print(f"ê²€ì¦ ê¸°ê°„: {validation_start_date} ~ {validation_end_date}")
    print(f"ë§ˆì§€ë§‰ ë‚ ì§œ({validation_end_date}) ê¸°ì¤€ìœ¼ë¡œë§Œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_predictions = []
    
    # ê° ì¢…ëª©ë³„ë¡œ í•œ ë²ˆë§Œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬
    for idx, row in tqdm(enumerate(stock_items.itertuples(index=True)), total=total_stock_items, desc="ì¢…ëª©ë³„ ê²€ì¦"):
        table_name = row.stock_name
        print(f"\nProcessing {table_name} ({idx + 1}/{total_stock_items})")
        
        # ë§ˆì§€ë§‰ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 1200ì¼ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
        start_date_1200 = validation_end_date - timedelta(days=1200)
        df = load_daily_craw_data(craw_db, table_name, start_date_1200, validation_end_date)
        
        if df.empty:
            print(f"No data found for {table_name}")
            continue
            
        print(f"Data loaded for {table_name}: {len(df)} rows")
        
        # íŠ¹ì„± ì¶”ì¶œ - í•œ ë²ˆë§Œ ìˆ˜í–‰
        df_features = extract_features(df, settings['COLUMNS_CHART_DATA'])
        if df_features.empty:
            print(f"Failed to extract features for {table_name}")
            continue
            
        # ë§ˆì§€ë§‰ 500ë´‰ë§Œ ì‚¬ìš©
        if len(df_features) > 500:
            df_features = df_features[-500:].copy()
            
        # 5ë´‰ í‰ê·  ê±°ë˜ëŸ‰ í™•ì¸ (5ë§Œ ì´í•˜ë©´ ì œì™¸)
        if 'Volume_MA5' in df_features.columns:
            last_row = df_features.iloc[-1]  # ê°€ì¥ ìµœê·¼ ë°ì´í„°
            if last_row['Volume_MA5'] <= 50000:
                print(f"Skipping {table_name}: 5-day average volume ({last_row['Volume_MA5']:.0f}) is below 50,000")
                continue
        
        # íŒ¨í„´ ì˜ˆì¸¡ - ë§ˆì§€ë§‰ 500ë´‰ ë°ì´í„°ë¡œ í•œ ë²ˆë§Œ ìˆ˜í–‰
        result, score = predict_pattern_with_score(model, df_features, table_name, use_data_dates=False, settings=settings)
        
        if not result.empty:
            # validation ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
            result = result[(result['date'] >= validation_start_date) & (result['date'] <= validation_end_date)]
            if not result.empty:
                all_predictions.append(result)
    
    # ëª¨ë“  ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if all_predictions:
        all_predictions_df = pd.concat(all_predictions, ignore_index=True)
    else:
        all_predictions_df = pd.DataFrame(columns=['date', 'stock_name', 'Score'])
    
    # ë‚ ì§œë³„ ë° ì¢…ëª©ë³„ë¡œ ë¶„ë¥˜
    date_grouped_predictions = {}
    
    if not all_predictions_df.empty:
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ê° ë‚ ì§œì—ì„œ ìƒìœ„ ì ìˆ˜ ì¢…ëª©ë§Œ ì„ íƒ
        for date, group in all_predictions_df.groupby('date'):
            top_stocks = group.nlargest(3, 'Score')  # ê° ë‚ ì§œë³„ ìƒìœ„ 3ê°œ ì„ íƒ
            date_grouped_predictions[date] = top_stocks
    
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    final_results = []
    for date, stocks in date_grouped_predictions.items():
        rank = 1
        for _, row in stocks.iterrows():
            final_results.append({
                'date': date,
                'stock_name': row['stock_name'],
                'score': round(row['Score'], 4),
                'rank': rank
            })
            rank += 1
    
    validation_results = pd.DataFrame(final_results)
    
    if not validation_results.empty:
        validation_results = validation_results.sort_values(by=['date', 'rank'])
        print("\nValidation results (Top 3 stocks by date):")
        print(validation_results)
        
        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        print("\nSummary by date:")
        for date, group in validation_results.groupby('date'):
            print(f"\nDate: {date.strftime('%Y-%m-%d')}")
            for _, row in group.iterrows():
                print(f"  Rank {row['rank']}: {row['stock_name']} (Score: {row['score']:.4f})")
        
        # ê²€ì¦ëœ ì¢…ëª©ì˜ ê°œìˆ˜ ì¶œë ¥
        unique_stock_names = validation_results['stock_name'].nunique()
        print(f"\nNumber of unique stock codes found during validation: {unique_stock_names}")
        
        # Validation ëë‚œ í›„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë³´ë‚´ê¸°
        message = "Validation completed. Top 3 stocks by date: dense_xgboost_auto\n\n"
        for date, group in validation_results.groupby('date'):
            message += f"ğŸ“… {date.strftime('%Y-%m-%d')}:\n"
            for _, row in group.iterrows():
                message += f"  #{row['rank']} {row['stock_name']} (Score: {row['score']:.4f})\n"
            message += "\n"
        
        message += f"Total unique dates: {validation_results['date'].nunique()}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    else:
        message = f"No patterns found in the validation period\n{results_table}\n{validation_start_date} to {validation_end_date}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return validation_results


def predict_pattern_with_score(model, df, stock_name, use_data_dates=False, settings=None):
    """
    íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ê³  ì˜ˆì¸¡ ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    try:
        print('Predicting patterns')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
            
        X = df[COLUMNS_TRAINING_DATA]
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(X)
        
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ì ìˆ˜ë¡œ ì‚¬ìš©)
        if hasattr(model, 'predict_proba'):
            # í´ë˜ìŠ¤ 1, 2, 3ì— ëŒ€í•œ í™•ë¥  ê³„ì‚° (í´ë˜ìŠ¤ 0 ì œì™¸)
            prediction_probs = model.predict_proba(X)
            
            # í´ë˜ìŠ¤ë³„ë¡œ ê°€ì¤‘ì¹˜ ì ìš© (í´ë˜ìŠ¤ ë²ˆí˜¸ì— ë¹„ë¡€)
            # í´ë˜ìŠ¤ 1: 1ë°°, í´ë˜ìŠ¤ 2: 2ë°°, í´ë˜ìŠ¤ 3: 3ë°°
            weighted_probs = np.zeros_like(prediction_probs[:, 1:])
            for i in range(prediction_probs.shape[1] - 1):  # í´ë˜ìŠ¤ 0 ì œì™¸
                class_idx = i + 1  # í´ë˜ìŠ¤ ë²ˆí˜¸ (1, 2, 3)
                weighted_probs[:, i] = prediction_probs[:, class_idx] * class_idx
            
            # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í™•ë¥ ì˜ í•©ê³„ë¥¼ ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = np.sum(weighted_probs, axis=1)
            print(f"Using weighted scoring: class 1(x1), class 2(x2), class 3(x3)")
            
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ì ìˆ˜ë¡œ ì‚¬ìš©)
        # if hasattr(model, 'predict_proba'):
        #     # í´ë˜ìŠ¤ 1, 2, 3ì— ëŒ€í•œ í™•ë¥  í•©ì‚° (í´ë˜ìŠ¤ 0 ì œì™¸)
        #     prediction_probs = model.predict_proba(X)
        #     # í´ë˜ìŠ¤ 0ì„ ì œì™¸í•œ ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤(1,2,3)ì˜ í™•ë¥  í•©ì‚°
        #     scores = np.sum(prediction_probs[:, 1:], axis=1)
        
        else:
            # predict_probaê°€ ì—†ìœ¼ë©´ ì˜ˆì¸¡ê°’ì„ ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = predictions

        df = df.loc[X.index]  # ë™ì¼í•œ ì¸ë±ìŠ¤ ìœ ì§€
        df['Prediction'] = predictions
        df['Score'] = scores
        
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > 0).sum()} matches found')
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        try:
            if df['date'].dtype == 'object':
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            df = df.dropna(subset=['date'])
            
            # ê²€ì¦ ê¸°ê°„ ì„¤ì •
            if use_data_dates:
                max_date = df['date'].max()
                validation_start_date = max_date + pd.Timedelta(days=1)
                validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
            else:
                validation_start_date = pd.to_datetime(cf.VALIDATION_START_DATE_AUTO)
                validation_end_date = pd.to_datetime(cf.VALIDATION_END_DATE_AUTO)
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ íŒ¨í„´ í•„í„°ë§
            recent_patterns = df[
                (df['Prediction'] > 0) & 
                (df['date'] >= validation_start_date) & 
                (df['date'] <= validation_end_date)
            ].copy()
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íŒ¨í„´ ì„ íƒ
            if not recent_patterns.empty:
                # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ íŒ¨í„´ ì„ íƒ
                best_pattern = recent_patterns.loc[recent_patterns['Score'].idxmax()]
                best_score = best_pattern['Score']
                
                recent_patterns['stock_name'] = stock_name
                result = recent_patterns[['date', 'stock_name', 'Score']]
                
                # ê²°ê³¼ì™€ ìµœê³  ì ìˆ˜ ë°˜í™˜
                return result, best_score
            else:
                return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
                
        except Exception as e:
            print(f"Error in date processing: {e}")
            return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
            
    except Exception as e:
        print(f'Error predicting patterns: {e}')
        return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ì„¤ì •
    buy_list_db, craw_db, settings = setup_environment()
    
    # ë°ì´í„° ë¡œë“œ
    filtered_results = load_filtered_stock_results(buy_list_db, settings['results_table'])
    
    if filtered_results.empty:
        print("Error: No filtered stock results loaded")
        return
    
    print("Filtered stock results loaded successfully")
    
    # ëª¨ë¸ íŒŒì¼ ì´ë¦„ ì €ì¥ ë³€ìˆ˜
    model_filename = None
    # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” í›ˆë ¨ ì„ íƒ
    best_model, best_accuracy, retrain = load_or_train_model(buy_list_db, craw_db, filtered_results, settings)
    
    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    print(f"Main function received: best_model={best_model is not None}, best_accuracy={best_accuracy}, retrain={retrain}")
    
    # ëª¨ë¸ í›ˆë ¨ (í•„ìš”í•œ ê²½ìš°)
    if retrain:
        print("Retrain flag is True. Starting model training...")
        best_model, best_accuracy = train_models(buy_list_db, craw_db, filtered_results, settings)
        
        # ëª¨ë¸ ì €ì¥ - retrainì´ Trueì¼ ë•Œë§Œ ì €ì¥
        if best_model:
            save_model(best_model, best_accuracy, settings)
        else:
            print("Warning: No model was returned from train_models function!")
    
    # ëª¨ë¸ ê²€ì¦
    validation_results = validate_model(best_model, buy_list_db, craw_db, settings)
    
    # ì„±ëŠ¥ í‰ê°€
    evaluate_model_performance(validation_results, buy_list_db, craw_db, settings, model_filename)

if __name__ == '__main__':
    main()