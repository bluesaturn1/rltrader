import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import f1_score, make_scorer
import os
import joblib
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
import cf
from mysql_loader import list_tables_in_database, load_data_from_mysql
from stock_utils import get_stock_items  # get_stock_items í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from tqdm import tqdm  # tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from telegram_utils import send_telegram_message  # í…”ë ˆê·¸ë¨ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from datetime import datetime, timedelta
from imblearn.over_sampling import SMOTE
from db_connection import DBConnectionManager
import validation_utils


def execute_update_query(self, query):
    """
    INSERT, UPDATE, DELETE ì¿¼ë¦¬ì™€ ê°™ì€ ë°ì´í„° ìˆ˜ì • ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    try:
        with self.engine.connect() as conn:
            conn.execute(text(query))
            conn.commit()
        return True
    except Exception as e:
        print(f"Query execution error: {e}")
        return False

def load_filtered_stock_results(db_manager, table):
    try:
        query = f"SELECT * FROM {table}"
        df = db_manager.execute_query(query)
        return df
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        return pd.DataFrame()

def load_daily_craw_data(db_manager, table, start_date, end_date):
    try:
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')
        print(f"Loading data from {start_date_str} to {end_date_str} for table {table}")
        
        query = f"""
            SELECT * FROM `{table}`
            WHERE date >= '{start_date_str}' AND date <= '{end_date_str}'
            ORDER BY date ASC
        """
        
        df = db_manager.execute_query(query)
        print(f"Data loaded from {start_date_str} to {end_date_str} for table {table}: {len(df)} rows")
        return df
    except Exception as e:
        print(f"Error loading data from MySQL: {e}")
        return pd.DataFrame()

def extract_features(df, COLUMNS_CHART_DATA):
    try:
        print(f'Original data rows: {len(df)}')
        print('Extracting features')

        # í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
        df = df[COLUMNS_CHART_DATA].copy()
        # ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
        df = df.sort_values(by='date')


        # ì´ë™í‰ê·  ê³„ì‚°
        df['MA5'] = df['close'].rolling(window=5).mean()
        df['MA10'] = df['close'].rolling(window=10).mean()
        df['MA20'] = df['close'].rolling(window=20).mean()
        df['MA60'] = df['close'].rolling(window=60).mean()
        df['MA120'] = df['close'].rolling(window=120).mean()
        df['MA240'] = df['close'].rolling(window=240).mean()
        
        # ì´ë™í‰ê· ê³¼ ì¢…ê°€ì˜ ë¹„ìœ¨ ê³„ì‚°
        df['Close_to_MA5'] = df['close'] / df['MA5']
        df['Close_to_MA10'] = df['close'] / df['MA10']
        df['Close_to_MA20'] = df['close'] / df['MA20']
        df['Close_to_MA60'] = df['close'] / df['MA60']
        df['Close_to_MA120'] = df['close'] / df['MA120']
        df['Close_to_MA240'] = df['close'] / df['MA240']
        
        # ê±°ë˜ëŸ‰ ì´ë™í‰ê·  ê³„ì‚°
        df['Volume_MA5'] = df['volume'].rolling(window=5).mean()
        df['Volume_MA10'] = df['volume'].rolling(window=10).mean()
        df['Volume_MA20'] = df['volume'].rolling(window=20).mean()
        df['Volume_MA60'] = df['volume'].rolling(window=60).mean()
        df['Volume_MA120'] = df['volume'].rolling(window=120).mean()
        df['Volume_MA240'] = df['volume'].rolling(window=240).mean()
        
        # ê±°ë˜ëŸ‰ê³¼ ì´ë™í‰ê· ì˜ ë¹„ìœ¨ ê³„ì‚°
        df['Volume_to_MA5'] = df['volume'] / df['Volume_MA5']
        df['Volume_to_MA10'] = df['volume'] / df['Volume_MA10']
        df['Volume_to_MA20'] = df['volume'] / df['Volume_MA20']
        df['Volume_to_MA60'] = df['volume'] / df['Volume_MA60']
        df['Volume_to_MA120'] = df['volume'] / df['Volume_MA120']
        df['Volume_to_MA240'] = df['volume'] / df['Volume_MA240']
        
        # ì¶”ê°€ ë¹„ìœ¨ ê³„ì‚°
        df['Open_to_LastClose'] = df['open'] / df['close'].shift(1)
        df['Close_to_LastClose'] = df['close'] / df['close'].shift(1)
        df['High_to_Close'] = df['high'] / df['close']
        df['Low_to_Close'] = df['low'] / df['close']
        
        df['Volume_to_LastVolume'] = df['volume'] / df['volume'].shift(1)
        
        df = df.dropna()
        print(f'Features extracted: {len(df)}')
        # print(df.head())
        # print(df.tail())
        return df
    except Exception as e:
        print(f'Error extracting features: {e}')
        return pd.DataFrame()

def label_data(df, signal_dates):
    try:
        print('Labeling data')
        
        df['Label'] = 0  # ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
        df['date'] = pd.to_datetime(df['date']).dt.date  # ë‚ ì§œ í˜•ì‹ì„ datetime.dateë¡œ ë³€í™˜
        
        # signal_datesë¥¼ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì˜ëª»ëœ í˜•ì‹ì˜ ë‚ ì§œë¥¼ ì²˜ë¦¬
        valid_signal_dates = []
        for date in signal_dates:
            try:
                valid_date = pd.to_datetime(date).date()
                valid_signal_dates.append(valid_date)
            except ValueError:
                print(f"Invalid date format: {date}")
        
        # ë‚ ì§œ ì •ë ¬
        valid_signal_dates.sort()
        print(f'Signal dates: {valid_signal_dates}')
        
        if len(valid_signal_dates) > 0:
            # 3ê°œì›”(ì•½ 90ì¼) ì´ìƒ ì°¨ì´ë‚˜ëŠ” ë‚ ì§œë¡œ ê·¸ë£¹ ë¶„í• 
            date_groups = []
            current_group = [valid_signal_dates[0]]
            
            for i in range(1, len(valid_signal_dates)):
                days_diff = (valid_signal_dates[i] - valid_signal_dates[i-1]).days
                if days_diff >= 90:  # 3ê°œì›” ì´ìƒ ì°¨ì´
                    date_groups.append(current_group)
                    current_group = [valid_signal_dates[i]]
                else:
                    current_group.append(valid_signal_dates[i])
            
            date_groups.append(current_group)
            
            print(f"Found {len(date_groups)} separate signal groups")
            
            # ê° ê·¸ë£¹ ì²˜ë¦¬
            for group_idx, group in enumerate(date_groups):
                print(f"Processing group {group_idx+1} with {len(group)} signals")
                
                # ê·¸ë£¹ì˜ ì‹œì‘ê³¼ ë ë‚ ì§œ
                start_date = min(group)
                end_date = max(group)
                
                # ì›ë˜ ì‹ í˜¸ ë‚ ì§œë“¤ì„ 3ë“±ë¶„í•˜ì—¬ ë¼ë²¨ ë¶€ì—¬
                n = len(group)
                first_third = group[:n//3] if n > 2 else group
                second_third = group[n//3:2*n//3] if n > 2 else []
                last_third = group[2*n//3:] if n > 2 else []
                
                # ì›ë³¸ ì‹ í˜¸ ë‚ ì§œì— ë¼ë²¨(1,2,3) ë¶€ì—¬
                signal_labels = {}
                for date in first_third:
                    signal_labels[date] = 1
                for date in second_third:
                    signal_labels[date] = 2
                for date in last_third:
                    signal_labels[date] = 3
                
                # ê° ì‹ í˜¸ ë‚ ì§œë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì ìš©
                sorted_dates = df[(df['date'] >= start_date) & (df['date'] <= end_date)]['date'].unique()
                sorted_dates.sort()
                
                # ê° ë‚ ì§œì— ëŒ€í•´ ì²˜ë¦¬
                current_label = 0
                for date in sorted_dates:
                    if date in signal_labels:
                        # ì‹ í˜¸ ë‚ ì§œì¸ ê²½ìš° í•´ë‹¹ ë¼ë²¨ë¡œ ì„¤ì •
                        current_label = signal_labels[date]
                    
                    # í˜„ì¬ ë¼ë²¨(ì´ì „ ì‹ í˜¸ì™€ ê°™ì€ ë¼ë²¨)ì„ ì ìš©
                    df.loc[df['date'] == date, 'Label'] = current_label
        
        print(f'Data labeled: {len(df)} rows')

        # ë¼ë²¨ ë¶„í¬ ì¶œë ¥
        print("Label distribution:")
        print(df['Label'].value_counts())
        
        # ì²« 5ê°œì™€ ë§ˆì§€ë§‰ 10ê°œì˜ ë¼ë²¨ ì¶œë ¥
        print("First 5 labels:")
        print(df[['date', 'Label']].head(3))
        print("Last 10 labels:")
        print(df[['date', 'Label']].tail(15))

        return df
    except Exception as e:
        print(f'Error labeling data: {e}')
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ traceback ì •ë³´ ì¶œë ¥
        return pd.DataFrame()

def train_model(X, y, use_saved_params=True, param_file='best_params.pkl'):
    try:
        print('Training model')
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        y = y[X.index]
        
        print("Class distribution in y:")
        print(y.value_counts())
        
        # Calculate class weights
        class_weights = {0: 1, 1: 1, 2: 1, 3: 1}  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
        for class_label, weight in y.value_counts(normalize=True).items():
            class_weights[class_label] = weight
        
        sample_weights = np.array([1/class_weights[yi] for yi in y])
        
        print(f"use_saved_params: {use_saved_params}")  # use_saved_params ê°’ ì¶œë ¥
        print(f"param_file exists: {os.path.exists(param_file)}")  # param_file ì¡´ì¬ ì—¬ë¶€ ì¶œë ¥
        
        if use_saved_params and os.path.exists(param_file):
            print("Loading saved parameters...")
            try:
                best_params = joblib.load(param_file)
                model = xgb.XGBClassifier(
                    **best_params,
                    random_state=42,
                    objective='multi:softmax',
                    num_class=4,
                    eval_metric='mlogloss'
                )
                print("Model loaded with saved parameters.")
            except Exception as e:
                print(f"Error loading saved parameters: {e}")
                model = None  # ë¡œë”© ì‹¤íŒ¨ ì‹œ modelì„ Noneìœ¼ë¡œ ì„¤ì •
        else:
            print("Tuning hyperparameters...")
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 1.0],
                'colsample_bytree': [0.8, 1.0],
                'min_child_weight': [1, 3, 5]
            }
            
            base_model = xgb.XGBClassifier(
                random_state=42,
                objective='multi:softmax',
                num_class=4,
                eval_metric='mlogloss'
            )
            
            grid_search = GridSearchCV(
                estimator=base_model,
                param_grid=param_grid,
                cv=3,
                scoring='f1_weighted',
                n_jobs=-1,
                verbose=2
            )
            
            # Train with sample weights
            grid_search.fit(X, y, sample_weight=sample_weights)
            
            best_params = grid_search.best_params_
            print(f'Best parameters found: {best_params}')
            joblib.dump(best_params, param_file)
            model = grid_search.best_estimator_
            print("Model trained with new parameters.")
        
        if model is not None:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Calculate weights for training set
            train_class_weights = {0: 1, 1: 1, 2: 1, 3: 1}  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
            for class_label, weight in y_train.value_counts(normalize=True).items():
                train_class_weights[class_label] = weight
            
            train_weights = np.array([1/train_class_weights[yi] for yi in y_train])
            
            # Train final model with weights
            model.fit(X_train, y_train, sample_weight=train_weights)
            
            y_pred = model.predict(X_test)
            print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
            print('Classification Report:')
            print(classification_report(y_test, y_pred))
        else:
            print("Model training failed.")
        
        return model
    except Exception as e:
        print(f'Error training model: {e}')
        import traceback
        traceback.print_exc()  # ìƒì„¸í•œ traceback ì •ë³´ ì¶œë ¥
        return None

def predict_pattern(model, df, stock_name, use_data_dates=True, settings=None):
    # í•¨ìˆ˜ ë‚´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„¤ì •ì€ ì§€ì—­ ë³€ìˆ˜ë¡œ ì¶”ì¶œ
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    try:
        print('Predicting patterns')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'stock_name'])
        X = df[COLUMNS_TRAINING_DATA]  # ì§€ì—­ ë³€ìˆ˜ë¡œ ê°„ê²°í•˜ê²Œ ì‚¬ìš©
     
        # ë¬´í•œëŒ€ ê°’ì´ë‚˜ ë„ˆë¬´ í° ê°’ ì œê±°
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        predictions = model.predict(X)
        df = df.loc[X.index]  # ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€
        df['Prediction'] = predictions
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > 0).sum()} matches found')
        
        # ë‚ ì§œ í˜•ì‹ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        try:
            # MySQLì˜ YYYYMMDD í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            if df['date'].dtype == 'object':
                # YYYYMMDD í˜•ì‹ì˜ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # NaT ê°’ ì œê±°
            df = df.dropna(subset=['date'])
            print(f"Date range in data: {df['date'].min()} to {df['date'].max()}")
            
            # ê²€ì¦ ê¸°ê°„ ì„¤ì • ë¶€ë¶„ ìˆ˜ì •
            if use_data_dates:
                # ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œ ì´í›„ë¡œ ì˜ˆì¸¡ ê²€ì¦ ê¸°ê°„ ì„¤ì • (í›ˆë ¨ ì§í›„ ê²€ì¦ìš©)
                max_date = df['date'].max()
                validation_start_date = max_date + pd.Timedelta(days=1)
                validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
            else:
                # cf.pyì— ì„¤ì •ëœ ê²€ì¦ ê¸°ê°„ ì‚¬ìš© (ì™¸ë¶€ ê²€ì¦ìš©)
                validation_start_date = pd.to_datetime(cf.VALIDATION_START_DATE)
                validation_end_date = pd.to_datetime(cf.VALIDATION_END_DATE)
            
            print(f"Validation period: {validation_start_date} to {validation_end_date}")
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ íŒ¨í„´ í•„í„°ë§ (Predictionì´ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ)
            recent_patterns = df[
                (df['Prediction'] > 0) & 
                (df['date'] >= validation_start_date) & 
                (df['date'] <= validation_end_date)
            ].copy()
            
            print(f'Filtered patterns in validation period: {len(recent_patterns)}')
            
            if not recent_patterns.empty:
                recent_patterns['stock_name'] = stock_name
                result = recent_patterns[['date', 'stock_name']]
                print(f'Found patterns for {stock_name}:')
                print(result)
                return result
            else:
                print(f'No patterns found for {stock_name} in validation period')
                return pd.DataFrame(columns=['date', 'stock_name'])
                
        except Exception as e:
            print(f"Error in date processing: {e}")
            print(f"Debug info - df['date'] sample: {df['date'].head()}")
            print(f"Debug info - validation dates: {cf.VALIDATION_END_DATE}")
            return pd.DataFrame(columns=['date', 'stock_name'])
            
    except Exception as e:
        print(f'Error predicting patterns: {e}')
        print(f'Error type: {type(e).__name__}')
        import traceback
        print(f'Stack trace:\n{traceback.format_exc()}')
        return pd.DataFrame(columns=['date', 'stock_name'])

def setup_environment():
    """í™˜ê²½ ì„¤ì • ë° í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("Starting pattern recognition by xgboost...")
    
    # ê¸°ë³¸ ì„¤ì •
    host = cf.MYSQL_HOST
    user = cf.MYSQL_USER
    password = cf.MYSQL_PASSWORD
    database_buy_list = cf.MYSQL_DATABASE_BUY_LIST
    database_craw = cf.MYSQL_DATABASE_CRAW
    results_table = cf.FINDING_SKYROCKET_TABLE
    performance_table = cf.RECOGNITION_PERFORMANCE_TABLE
    telegram_token = cf.TELEGRAM_BOT_TOKEN
    telegram_chat_id = cf.TELEGRAM_CHAT_ID
    
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    model_dir = 'models'
    os.makedirs(model_dir, exist_ok=True)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì ìƒì„±
    buy_list_db = DBConnectionManager(host, user, password, database_buy_list)
    craw_db = DBConnectionManager(host, user, password, database_craw)

    # ì—´ ì •ì˜
    COLUMNS_CHART_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']
    COLUMNS_TRAINING_DATA = [
        'Open_to_LastClose', 'High_to_Close', 'Low_to_Close',
        'Close_to_LastClose', 'Volume_to_LastVolume',
        'Close_to_MA5', 'Volume_to_MA5',
        'Close_to_MA10', 'Volume_to_MA10',
        'Close_to_MA20', 'Volume_to_MA20',
        'Close_to_MA60', 'Volume_to_MA60',
        'Close_to_MA120', 'Volume_to_MA120',
        'Close_to_MA240', 'Volume_to_MA240',
    ]
    
    # ì„¤ì • ì‚¬ì „ ìƒì„±
    settings = {
        'host': host,
        'user': user,
        'password': password,
        'database_buy_list': database_buy_list,
        'database_craw': database_craw,
        'results_table': results_table,
        'performance_table': performance_table,
        'telegram_token': telegram_token,
        'telegram_chat_id': telegram_chat_id,   
        'COLUMNS_CHART_DATA': COLUMNS_CHART_DATA,
        'COLUMNS_TRAINING_DATA': COLUMNS_TRAINING_DATA,
        'model_dir': model_dir,
        'current_date': datetime.now().strftime('%Y%m%d'),
        'param_file': 'best_params.pkl',
        'buy_list_db': buy_list_db,  # ì¶”ê°€: buy_list_db ê°ì²´
        'craw_db': craw_db,  # ì¶”ê°€: craw_db ê°ì²´
        'model_name': 'xgboost_weighted'  # ì¶”ê°€: ëª¨ë¸ ì´ë¦„
    }
    
    return buy_list_db, craw_db, settings

def load_or_train_model(buy_list_db, craw_db, filtered_results, settings):
    """ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    model_dir = settings['model_dir']
    results_table = settings['results_table']
    current_date = settings['current_date']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    
    model_filename = os.path.join(model_dir, f"{results_table}_{current_date}.json")
    print(f"Model filename: {model_filename}")
    
    # ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ í›ˆë ¨ ì—¬ë¶€ ì§ˆë¬¸ (ê¸°ë³¸ê°’: 'no')
    choice = input("Do you want to retrain the model? (yes/no) [no]: ").strip().lower()
    if not choice:  # ì…ë ¥ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        choice = 'no'
    
    print(f"User choice: {choice}")
    
    if choice == 'yes':
        # ì‚¬ìš©ìê°€ 'ì˜ˆ'ë¥¼ ì„ íƒí•œ ê²½ìš° - ëª¨ë¸ ì¬í›ˆë ¨
        print("User selected to retrain the model.")
        print("Will proceed to train_models function...")
        return None, 0.0, True  # ëª¨ë¸ ì—†ìŒ, ì •í™•ë„ 0, retrain=True
    elif choice == 'no':
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_models = [f for f in os.listdir(model_dir) if f.endswith('.json')]
        
        if not available_models:
            print("No saved models found. Will train a new model.")
            return None, 0.0, True
        else:
            print("\nAvailable models:")
            for i, model_file in enumerate(available_models):
                print(f"{i+1}. {model_file}")
            
            # ì‚¬ìš©ìì—ê²Œ ëª¨ë¸ ì„ íƒ ìš”ì²­
            while True:
                try:
                    model_choice = input("\nSelect a model number (or type 'new' to train a new model): ")
                    
                    if model_choice.lower() == 'new':
                        print("User selected to train a new model.")
                        return None, 0.0, True
                    else:
                        model_index = int(model_choice) - 1
                        if 0 <= model_index < len(available_models):
                            model_filename = os.path.join(model_dir, available_models[model_index])
                            print(f"Loading model: {model_filename}")
                            model = xgb.XGBClassifier()
                            model.load_model(model_filename)
                            return model, 0.0, False  # ë¡œë“œí•œ ëª¨ë¸, ì •í™•ë„, retrain ì—¬ë¶€
                        else:
                            print("Invalid model number. Please try again.")
                except ValueError:
                    print("Invalid input. Please enter a number or 'new'.")
    else:
        # ì˜ëª»ëœ ì…ë ¥ì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ 'no' ì²˜ë¦¬
        print(f"Invalid choice: '{choice}'. Defaulting to 'no'.")
        return load_or_train_model(buy_list_db, craw_db, filtered_results, settings)  # ì¬ê·€ì ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸


def train_models(buy_list_db, craw_db, filtered_results, settings):
    """XGBoost ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    print("Retraining the model...")
    param_file = settings['param_file']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    # ì²« ë²ˆì§¸ ì¢…ëª©ì— ëŒ€í•´ì„œë§Œ use_saved_paramsë¥¼ Falseë¡œ ì„¤ì •
    first_stock = True
    best_model = None
    best_accuracy = 0
    total_models = 0
    successful_models = 0
    
    # ì¢…ëª©ë³„ë¡œ ê·¸ë£¹í™”
    grouped_results = filtered_results.groupby('stock_name')
    
    # ê° ê·¸ë£¹ì˜ ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ë©° ì¢…ëª©ë³„, ê·¸ë£¹ë³„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëª¨ë¸ì„ í›ˆë ¨
    for stock_name, group in tqdm(grouped_results, desc="Training models"):
        signal_dates = group['signal_date'].tolist()
        
        # ë¬¸ìì—´ í˜•íƒœì˜ signal_datesë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        valid_signal_dates = []
        for date in signal_dates:
            if isinstance(date, str):
                try:
                    valid_date = pd.to_datetime(date.strip(), format='%Y%m%d').date()
                    valid_signal_dates.append(valid_date)
                except ValueError:
                    print(f"Invalid date format: {date}")
            else:
                print(f"Invalid date type: {date}")
    
        if not valid_signal_dates:
            print(f"No valid signal dates for {stock_name}")
            continue
        
        # 3ê°œì›”(ì•½ 90ì¼) ì´ìƒ ì°¨ì´ë‚˜ëŠ” ë‚ ì§œë¡œ ê·¸ë£¹ ë¶„í• 
        date_groups = []
        current_group = [valid_signal_dates[0]]
        
        for i in range(1, len(valid_signal_dates)):
            days_diff = (valid_signal_dates[i] - valid_signal_dates[i-1]).days
            if days_diff >= 90:  # 3ê°œì›” ì´ìƒ ì°¨ì´
                date_groups.append(current_group)
                current_group = [valid_signal_dates[i]]
            else:
                current_group.append(valid_signal_dates[i])
        
        date_groups.append(current_group)
        
        # ê° ê·¸ë£¹ë³„ë¡œ ë³„ë„ ëª¨ë¸ í›ˆë ¨
        for group_idx, signal_group in enumerate(date_groups):
            end_date = max(signal_group)  # ê·¸ë£¹ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ
            start_date = end_date - timedelta(days=1200)
            
            print(f"\nTraining model for {stock_name} - Group {group_idx+1}: {start_date} to {end_date}")
            df = load_daily_craw_data(craw_db, stock_name, start_date, end_date)
            
            if df.empty:
                continue
                
            # íŠ¹ì„± ì¶”ì¶œ ë° ë¼ë²¨ë§
            df = extract_features(df, settings['COLUMNS_CHART_DATA'])
            df = label_data(df, signal_group)  # í•´ë‹¹ ê·¸ë£¹ì˜ ë‚ ì§œë§Œ ì „ë‹¬
            # 500ë´‰ë§Œ ì˜ë¼ì„œ í›ˆë ¨
            if len(df) > 500:
                df = df[-500:]
                
            # ëª¨ë¸ í›ˆë ¨
            X = df[COLUMNS_TRAINING_DATA]
            y = df['Label']
            model = train_model(X, y, use_saved_params=(not first_stock), param_file=param_file)
            
            # ëª¨ë¸ í‰ê°€ ë° ì €ì¥
            if model:
                # í›ˆë ¨ ì •ë³´ ì¶œë ¥
                print(f"Model trained for {stock_name} from {start_date} to {end_date}")
                
                # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•˜ê¸° ìœ„í•´ ì„±ëŠ¥ í‰ê°€
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                if accuracy > best_accuracy or best_model is None:
                    best_model = model
                    best_accuracy = accuracy
                    print(f"New best model found for {stock_name} with accuracy: {accuracy:.4f}")
            else:
                print(f"Model training failed for {stock_name}")
        
            total_models += 1
            if model:
                successful_models += 1
        
        # ì²« ë²ˆì§¸ ì¢…ëª© ì²˜ë¦¬ í›„ í”Œë˜ê·¸ ë³€ê²½
        first_stock = False
    
    print(f"\nTotal models trained: {total_models}")
    print(f"Successful models: {successful_models}")
    
    # í›ˆë ¨ì´ ëë‚œ í›„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë³´ë‚´ê¸°
    message = f"Training completed.\nTotal models trained: {total_models}\nSuccessful models: {successful_models}"
    send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return best_model, best_accuracy


def save_model(model, accuracy, settings):
    """í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    model_dir = settings['model_dir']
    results_table = settings['results_table']
    current_date = settings['current_date']
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    
    model_filename = os.path.join(model_dir, f"{results_table}_{current_date}.json")
    
    if model:
        print("Saving best model...")
        model.save_model(model_filename)
        print(f"Best model saved as {model_filename} with accuracy: {accuracy:.4f}")
        message = f"Best model saved as {model_filename} with accuracy: {accuracy:.4f}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    else:
        print("No model to save.")
        pause = input("Press Enter to continue...")
        message = "No model to save."
        send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return model_filename  # íŒŒì¼ ì´ë¦„ ë°˜í™˜


def validate_model(model, buy_list_db, craw_db, settings):
    """í•™ìŠµëœ ëª¨ë¸ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    telegram_token = settings['telegram_token']
    telegram_chat_id = settings['telegram_chat_id']
    results_table = settings['results_table']
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    # ê²€ì¦ ê¸°ê°„ ì„¤ì •
    print(f"\nLoading data for validation from {cf.VALIDATION_START_DATE} to {cf.VALIDATION_END_DATE}")
    validation_start_date = pd.to_datetime(cf.VALIDATION_START_DATE)
    validation_end_date = pd.to_datetime(cf.VALIDATION_END_DATE)
    
    # ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ê²€ì¦ ë°ì´í„° ë¡œë“œ
    stock_items = get_stock_items(settings['host'], settings['user'], settings['password'], settings['database_buy_list'])
    total_stock_items = len(stock_items)
    print(f"\nì „ì²´ ì¢…ëª© ìˆ˜: {total_stock_items}")
    print(f"ê²€ì¦ ê¸°ê°„: {validation_start_date} ~ {validation_end_date}")
    print(f"ë§ˆì§€ë§‰ ë‚ ì§œ({validation_end_date}) ê¸°ì¤€ìœ¼ë¡œë§Œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_predictions = []
    
    # ê° ì¢…ëª©ë³„ë¡œ í•œ ë²ˆë§Œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬
    for idx, row in tqdm(enumerate(stock_items.itertuples(index=True)), total=total_stock_items, desc="ì¢…ëª©ë³„ ê²€ì¦"):
        table_name = row.stock_name
        print(f"\nProcessing {table_name} ({idx + 1}/{total_stock_items})")
        
        # ë§ˆì§€ë§‰ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 1200ì¼ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
        start_date_1200 = validation_end_date - timedelta(days=1200)
        df = load_daily_craw_data(craw_db, table_name, start_date_1200, validation_end_date)
        
        if df.empty:
            print(f"No data found for {table_name}")
            continue
            
        print(f"Data loaded for {table_name}: {len(df)} rows")
        
        # íŠ¹ì„± ì¶”ì¶œ - í•œ ë²ˆë§Œ ìˆ˜í–‰
        df_features = extract_features(df, settings['COLUMNS_CHART_DATA'])
        if df_features.empty:
            print(f"Failed to extract features for {table_name}")
            continue
            
        # ë§ˆì§€ë§‰ 500ë´‰ë§Œ ì‚¬ìš©
        if len(df_features) > 500:
            df_features = df_features[-500:].copy()
            
        # 5ë´‰ í‰ê·  ê±°ë˜ëŸ‰ í™•ì¸ (5ë§Œ ì´í•˜ë©´ ì œì™¸)
        if 'Volume_MA5' in df_features.columns:
            last_row = df_features.iloc[-1]  # ê°€ì¥ ìµœê·¼ ë°ì´í„°
            if last_row['Volume_MA5'] <= 50000:
                print(f"Skipping {table_name}: 5-day average volume ({last_row['Volume_MA5']:.0f}) is below 50,000")
                continue
        
        # íŒ¨í„´ ì˜ˆì¸¡ - ë§ˆì§€ë§‰ 500ë´‰ ë°ì´í„°ë¡œ í•œ ë²ˆë§Œ ìˆ˜í–‰
        result, score = predict_pattern_with_score(model, df_features, table_name, use_data_dates=False, settings=settings)
        
        if not result.empty:
            # validation ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
            result = result[(result['date'] >= validation_start_date) & (result['date'] <= validation_end_date)]
            if not result.empty:
                all_predictions.append(result)
    
    # ëª¨ë“  ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if all_predictions:
        all_predictions_df = pd.concat(all_predictions, ignore_index=True)
    else:
        all_predictions_df = pd.DataFrame(columns=['date', 'stock_name', 'Score'])
    
    # ë‚ ì§œë³„ ë° ì¢…ëª©ë³„ë¡œ ë¶„ë¥˜
    date_grouped_predictions = {}
    
    if not all_predictions_df.empty:
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ê° ë‚ ì§œì—ì„œ ìƒìœ„ ì ìˆ˜ ì¢…ëª©ë§Œ ì„ íƒ
        for date, group in all_predictions_df.groupby('date'):
            top_stocks = group.nlargest(3, 'Score')  # ê° ë‚ ì§œë³„ ìƒìœ„ 3ê°œ ì„ íƒ
            date_grouped_predictions[date] = top_stocks
    
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    final_results = []
    for date, stocks in date_grouped_predictions.items():
        rank = 1
        for _, row in stocks.iterrows():
            final_results.append({
                'date': date,
                'stock_name': row['stock_name'],
                'score': round(row['Score'], 4),
                'rank': rank
            })
            rank += 1
    
    validation_results = pd.DataFrame(final_results)
    
    if not validation_results.empty:
        validation_results = validation_results.sort_values(by=['date', 'rank'])
        print("\nValidation results (Top 3 stocks by date):")
        print(validation_results)
        
        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        print("\nSummary by date:")
        for date, group in validation_results.groupby('date'):
            print(f"\nDate: {date.strftime('%Y-%m-%d')}")
            for _, row in group.iterrows():
                print(f"  Rank {row['rank']}: {row['stock_name']} (Score: {row['score']:.4f})")
        
        # ê²€ì¦ëœ ì¢…ëª©ì˜ ê°œìˆ˜ ì¶œë ¥
        unique_stock_names = validation_results['stock_name'].nunique()
        print(f"\nNumber of unique stock codes found during validation: {unique_stock_names}")
        
        # Validation ëë‚œ í›„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë³´ë‚´ê¸°
        message = "Validation completed. Top 3 stocks by date:\n\n"
        for date, group in validation_results.groupby('date'):
            message += f"ğŸ“… {date.strftime('%Y-%m-%d')}:\n"
            for _, row in group.iterrows():
                message += f"  #{row['rank']} {row['stock_name']} (Score: {row['score']:.4f})\n"
            message += "\n"
        
        message += f"Total unique dates: {validation_results['date'].nunique()}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    else:
        message = f"No patterns found in the validation period\n{results_table}\n{validation_start_date} to {validation_end_date}"
        send_telegram_message(telegram_token, telegram_chat_id, message)
    
    return validation_results


def predict_pattern_with_score(model, df, stock_name, use_data_dates=False, settings=None):
    """
    íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ê³  ì˜ˆì¸¡ ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    COLUMNS_TRAINING_DATA = settings['COLUMNS_TRAINING_DATA']
    
    try:
        print('Predicting patterns')
        if model is None:
            print("Model is None, cannot predict patterns.")
            return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
            
        X = df[COLUMNS_TRAINING_DATA]
        X = X.replace([np.inf, -np.inf], np.nan).dropna()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(X)
        
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ì ìˆ˜ë¡œ ì‚¬ìš©)
        if hasattr(model, 'predict_proba'):
            # í´ë˜ìŠ¤ 1, 2, 3ì— ëŒ€í•œ í™•ë¥  ê³„ì‚° (í´ë˜ìŠ¤ 0 ì œì™¸)
            prediction_probs = model.predict_proba(X)
            
            # í´ë˜ìŠ¤ë³„ë¡œ ê°€ì¤‘ì¹˜ ì ìš© (í´ë˜ìŠ¤ ë²ˆí˜¸ì— ë¹„ë¡€)
            # í´ë˜ìŠ¤ 1: 1ë°°, í´ë˜ìŠ¤ 2: 2ë°°, í´ë˜ìŠ¤ 3: 3ë°°
            weighted_probs = np.zeros_like(prediction_probs[:, 1:])
            for i in range(prediction_probs.shape[1] - 1):  # í´ë˜ìŠ¤ 0 ì œì™¸
                class_idx = i + 1  # í´ë˜ìŠ¤ ë²ˆí˜¸ (1, 2, 3)
                weighted_probs[:, i] = prediction_probs[:, class_idx] * class_idx
            
            # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í™•ë¥ ì˜ í•©ê³„ë¥¼ ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = np.sum(weighted_probs, axis=1)
            print(f"Using weighted scoring: class 1(x1), class 2(x2), class 3(x3)")
            
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ì ìˆ˜ë¡œ ì‚¬ìš©)
        # if hasattr(model, 'predict_proba'):
        #     # í´ë˜ìŠ¤ 1, 2, 3ì— ëŒ€í•œ í™•ë¥  í•©ì‚° (í´ë˜ìŠ¤ 0 ì œì™¸)
        #     prediction_probs = model.predict_proba(X)
        #     # í´ë˜ìŠ¤ 0ì„ ì œì™¸í•œ ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤(1,2,3)ì˜ í™•ë¥  í•©ì‚°
        #     scores = np.sum(prediction_probs[:, 1:], axis=1)
        
        else:
            # predict_probaê°€ ì—†ìœ¼ë©´ ì˜ˆì¸¡ê°’ì„ ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = predictions

        df = df.loc[X.index]  # ë™ì¼í•œ ì¸ë±ìŠ¤ ìœ ì§€
        df['Prediction'] = predictions
        df['Score'] = scores
        
        print(f'Patterns predicted: {len(predictions)} total predictions')
        print(f'Patterns with value > 0: {(predictions > 0).sum()} matches found')
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        try:
            if df['date'].dtype == 'object':
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
            elif not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            df = df.dropna(subset=['date'])
            
            # ê²€ì¦ ê¸°ê°„ ì„¤ì •
            if use_data_dates:
                max_date = df['date'].max()
                validation_start_date = max_date + pd.Timedelta(days=1)
                validation_end_date = validation_start_date + pd.Timedelta(days=cf.PREDICTION_VALIDATION_DAYS)
            else:
                validation_start_date = pd.to_datetime(cf.VALIDATION_START_DATE)
                validation_end_date = pd.to_datetime(cf.VALIDATION_END_DATE)
            
            # ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ íŒ¨í„´ í•„í„°ë§
            recent_patterns = df[
                (df['Prediction'] > 0) & 
                (df['date'] >= validation_start_date) & 
                (df['date'] <= validation_end_date)
            ].copy()
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íŒ¨í„´ ì„ íƒ
            if not recent_patterns.empty:
                # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ íŒ¨í„´ ì„ íƒ
                best_pattern = recent_patterns.loc[recent_patterns['Score'].idxmax()]
                best_score = best_pattern['Score']
                
                recent_patterns['stock_name'] = stock_name
                result = recent_patterns[['date', 'stock_name', 'Score']]
                
                # ê²°ê³¼ì™€ ìµœê³  ì ìˆ˜ ë°˜í™˜
                return result, best_score
            else:
                return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
                
        except Exception as e:
            print(f"Error in date processing: {e}")
            return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0
            
    except Exception as e:
        print(f'Error predicting patterns: {e}')
        return pd.DataFrame(columns=['date', 'stock_name', 'Score']), 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ì„¤ì •
    buy_list_db, craw_db, settings = setup_environment()
    
    # ë°ì´í„° ë¡œë“œ
    filtered_results = load_filtered_stock_results(buy_list_db, settings['results_table'])
    
    if filtered_results.empty:
        print("Error: No filtered stock results loaded")
        return
    
    print("Filtered stock results loaded successfully")
    
    # ëª¨ë¸ íŒŒì¼ ì´ë¦„ ì €ì¥ ë³€ìˆ˜
    model_filename = None
    # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” í›ˆë ¨ ì„ íƒ
    best_model, best_accuracy, retrain = load_or_train_model(buy_list_db, craw_db, filtered_results, settings)
    
    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    print(f"Main function received: best_model={best_model is not None}, best_accuracy={best_accuracy}, retrain={retrain}")
    
    # ëª¨ë¸ í›ˆë ¨ (í•„ìš”í•œ ê²½ìš°)
    if retrain:
        print("Retrain flag is True. Starting model training...")
        best_model, best_accuracy = train_models(buy_list_db, craw_db, filtered_results, settings)
        
        # ëª¨ë¸ ì €ì¥ - retrainì´ Trueì¼ ë•Œë§Œ ì €ì¥
        if best_model:
            save_model(best_model, best_accuracy, settings)
        else:
            print("Warning: No model was returned from train_models function!")
    
    # ëª¨ë¸ ê²€ì¦
    validation_results = validate_model(best_model, buy_list_db, craw_db, settings)
    
    # ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ì²˜ë¦¬ (settingsì— ì´ë¯¸ craw_dbê°€ í¬í•¨ë˜ì–´ ìˆìŒ)
    validation_utils.process_and_report_validation_results(validation_results, settings)

if __name__ == '__main__':
    main()