import pandas as pd
from telegram_utils import send_telegram_message, send_long_telegram_message  # í…”ë ˆê·¸ë¨ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from tqdm import tqdm
from stock_utils import load_daily_craw_data  

def process_and_report_validation_results(validation_results, settings):
    """
    Processes validation results, calculates performance, saves data, and sends Telegram messages.

    Args:
        validation_results (pd.DataFrame): DataFrame containing validation results.
        settings (dict): Dictionary containing settings (e.g., database credentials).
    """
    if validation_results.empty:
        print("No validation results to process.")
        return

    # Convert column names to lowercase
    validation_results.columns = validation_results.columns.str.lower()

    # Create performance DataFrame
    performance_df = evaluate_performance(validation_results, settings['craw_db'])

    # Save performance_df to the deep_learning database
    save_xgboost_predictions_to_db(performance_df, settings)
    
    # Send validation summary
    send_validation_summary(validation_results, performance_df, settings)


def calculate_performance(df, start_date, end_date):
    try:
        print('Caluating performance')
        df['date'] = pd.to_datetime(df['date'])
        
        # ë‹¤ìŒë‚  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(ì˜¤ëŠ˜ì´ ë§ˆì§€ë§‰ ë‚ ì§œì¸ ê²½ìš°) ì²´í¬
        if df[df['date'] >= start_date].empty:
            print(f"No data available from {start_date} (next trading day). Returning 0.")
            return 0.0, 0.0, 0.0, 0.0  # ìµœëŒ€ ìˆ˜ìµë¥ , ìµœëŒ€ ì†ì‹¤ë¥ , ì˜ˆìƒ ìˆ˜ìµë¥ , ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ë°˜í™˜
        
        # ë§¤ìˆ˜ì¼(start_date)ì˜ ì¢…ê°€ ê°€ì ¸ì˜¤ê¸° - ë§¤ìˆ˜ê°€ê²© ì„¤ì •
        buy_data = df[df['date'] >= start_date].iloc[0]
        buy_price = buy_data['close']
        buy_date = buy_data['date']
        
        # ë§¤ìˆ˜ì¼ë¶€í„° 60ì¼ê°„ì˜ ë°ì´í„° ì„ íƒ
        period_df = df[(df['date'] >= buy_date) & (df['date'] <= end_date)]
        
        if period_df.empty or len(period_df) < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”
            print(f"Insufficient data between {buy_date} and {end_date}")
            return 0.0, 0.0, 0.0, 0.0  # ìµœëŒ€ ìˆ˜ìµë¥ , ìµœëŒ€ ì†ì‹¤ë¥ , ì˜ˆìƒ ìˆ˜ìµë¥ , ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ë°˜í™˜
        
        # ìµœëŒ€ ìˆ˜ìµë¥  ê³„ì‚° (ìµœê³ ê°€ ê¸°ì¤€)
        max_price = period_df['high'].max()
        max_profit_rate = (max_price - buy_price) / buy_price * 100
        
        # ìµœëŒ€ ì†ì‹¤ë¥  ê³„ì‚° (ìµœì €ê°€ ê¸°ì¤€)
        min_price = period_df['low'].min()
        max_loss_rate = (min_price - buy_price) / buy_price * 100  # ì†ì‹¤ì€ ìŒìˆ˜ë¡œ í‘œí˜„ë¨
        
        # ì˜ˆìƒ ìˆ˜ìµë¥  = ìµœëŒ€ ìˆ˜ìµë¥  - |ìµœëŒ€ ì†ì‹¤ë¥ |
        estimated_profit_rate = max_profit_rate - abs(max_loss_rate)
        
        # ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ê³„ì‚° (ì˜ˆì‹œ)
        risk_adjusted_return = estimated_profit_rate / abs(max_loss_rate)
        
        print(f"Buy price: {buy_price}, Max price: {max_price}, Min price: {min_price}")
        print(f"Max profit: {max_profit_rate:.2f}%, Max loss: {max_loss_rate:.2f}%, Estimated profit: {estimated_profit_rate:.2f}%, Risk-adjusted return: {risk_adjusted_return:.2f}%")
        
        return max_profit_rate, max_loss_rate, estimated_profit_rate, risk_adjusted_return
        
    except Exception as e:
        print(f'Error evaluating performance: {e}')
        import traceback
        traceback.print_exc()
        return 0.0, 0.0, 0.0, 0.0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0 ë°˜í™˜


def evaluate_performance(validation_results, craw_db):

    # í•„í„°ë§ëœ ë‚ ì§œ-ì¢…ëª© ì¡°í•©ì— ëŒ€í•´ ì„±ëŠ¥ í‰ê°€
    performance_results = []
    for index, row in tqdm(validation_results.iterrows(), total=len(validation_results), desc="Evaluating performance"):
        stock_name = row['stock_name']  # stock_name -> stock_name
        pattern_date = row['date']
        confidence = row.get('Confidence', 0)  # confidence ê°’ ê°€ì ¸ì˜¤ê¸°
        prediction = row.get('Prediction', 0) # prediction ê°’ ê°€ì ¸ì˜¤ê¸°
        performance_start_date = pattern_date + pd.Timedelta(days=1)  # ë‹¤ìŒë‚  ë§¤ìˆ˜
        performance_end_date = performance_start_date + pd.Timedelta(days=60)
        
        df = load_daily_craw_data(craw_db, stock_name, performance_start_date, performance_end_date)
        print(f"Evaluating performance for {stock_name} from {performance_start_date} to {performance_end_date}: {len(df)} rows")
        
        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ê²°ê³¼ì— í¬í•¨ (ë§ˆì§€ë§‰ ë‚ ì§œ ì²˜ë¦¬ë¥¼ ìœ„í•¨)
        if df.empty:
            print(f"No data available for {stock_name} after {pattern_date}. Including with 0 return.")
            performance_results.append({
                'stock_name': stock_name,  # stock_name -> stock_name
                'pattern_date': pattern_date,
                'start_date': performance_start_date,
                'end_date': performance_end_date,
                'max_return': 0.0,  # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0 ë°˜í™˜
                'max_loss': 0.0,  # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0 ë°˜í™˜
                'estimated_profit_rate': 0.0,  # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0 ë°˜í™˜
                'risk_adjusted_return': 0.0, # risk_adjusted_return ê°’ ì¶”ê°€
                'confidence': confidence,  # confidence ê°’ ì €ì¥
                'prediction': prediction # prediction ê°’ ì €ì¥
            })
        else:
            max_return, max_loss, estimated_profit_rate, risk_adjusted_return = calculate_performance(df, performance_start_date, performance_end_date)
            
            # Noneì´ ë°˜í™˜ë˜ëŠ” ê²½ìš°ì—ë„ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í¬í•¨
            if max_return is None:
                max_return = 0.0
            if max_loss is None:
                max_loss = 0.0
            if estimated_profit_rate is None:
                estimated_profit_rate = 0.0
            if risk_adjusted_return is None:
                risk_adjusted_return = 0.0
                
            performance_results.append({
                'stock_name': stock_name,  # stock_name -> stock_name
                'pattern_date': pattern_date,
                'start_date': performance_start_date,
                'end_date': performance_end_date,
                'max_return': round(max_return, 2),  # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                'max_loss': round(max_loss, 2),  # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                'estimated_profit_rate': round(estimated_profit_rate, 2),  # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                'risk_adjusted_return': round(risk_adjusted_return, 2), # risk_adjusted_return ê°’ ì¶”ê°€
                'confidence': round(confidence, 4),   # confidence ê°’ ì €ì¥
                'prediction': round(prediction, 4) # prediction ê°’ ì €ì¥
            })
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (index + 1) % 10 == 0 or (index + 1) == len(validation_results):
            print(f"Evaluated performance for {index + 1}/{len(validation_results)} patterns")
    
    performance_df = pd.DataFrame(performance_results)
    return performance_df

def send_validation_summary(validation_results, performance_df, settings):

    print("\n=== ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===")
    
    # ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    total_predictions = len(validation_results)
    total_performance = len(performance_df)
    print(f"ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}")
    print(f"ì„±ê³¼ í‰ê°€ ìˆ˜: {total_performance}")
    
    if not performance_df.empty:

        avg_return = performance_df['estimated_profit_rate'].mean()
        avg_risk_adjusted_return = performance_df['risk_adjusted_return'].mean()
        max_return = performance_df['max_return'].max()
        max_loss = performance_df['max_loss'].min()

        print(f"í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {avg_return:.2f}%")
        print(f"ìµœê³  ìˆ˜ìµë¥ : {max_return:.2f}%")
        print(f"ìµœê³  ì†ì‹¤ë¥ : {max_loss:.2f}%")
        
        # ë‚ ì§œë³„ ìƒìœ„ ì¢…ëª© ë¶„ì„
        try:
            results, summaries = analyze_top_performers_by_date(performance_df, top_n=3)
            
            # ì—¬ëŸ¬ ë‚ ì§œë¥¼ ëª¨ì•„ì„œ ë³´ë‚´ê¸° ìœ„í•œ ë³€ìˆ˜ë“¤
            batch_size = 8  # í•œ ë²ˆì— ë³´ë‚¼ ë‚ ì§œ ìˆ˜ (5 ë˜ëŠ” 7ë¡œ ì„¤ì •)
            messages_batch = []
            batch_counter = 0
            
            for i, result in enumerate(results):
                date = result['date']
                top_stocks = result['top_stocks']
                
                # í˜„ì¬ ë‚ ì§œ ì •ë³´ ë©”ì‹œì§€ ìƒì„±
                current_message = f"\nğŸ“…ë‚ ì§œ: {date}\n"
                current_message += "ì¢…ëª©ëª… | Confidence | ìµœëŒ€ ìˆ˜ìµë¥  | ìµœëŒ€ ì†ì‹¤ | ì˜ˆìƒ ìˆ˜ìµë¥  | ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥ \n"
                for _, row in top_stocks.iterrows():
                    current_message += (
                        f"{row['stock_name']} | {row['confidence']:.4f} | "
                        f"{row['max_return']:.2f}% | {row['max_loss']:.2f}% | "
                        f"{row['estimated_profit_rate']:.2f}% | {row['risk_adjusted_return']:.2f}%\n"
                    )
                
                # ë°°ì¹˜ì— í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
                messages_batch.append(current_message)
                batch_counter += 1
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ê²°ê³¼ì¸ ê²½ìš° ë©”ì‹œì§€ ì „ì†¡
                if batch_counter >= batch_size or i == len(results) - 1:
                    # ë°°ì¹˜ ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡
                    batch_message = "=== ë‚ ì§œë³„ ìƒìœ„ 3ê°œ ì¢…ëª© ===\n" + "\n".join(messages_batch)
                    send_telegram_message(settings['telegram_token'], settings['telegram_chat_id'], batch_message)
                    
                    # ë°°ì¹˜ ì´ˆê¸°í™”
                    messages_batch = []
                    batch_counter = 0
            
            # ê²€ì¦ ê²°ê³¼ ìš”ì•½ ë©”ì‹œì§€ ë³„ë„ë¡œ ì „ì†¡
            summary_message = ("\n=== ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===\n"
                f"ëª¨ë¸ : {settings['model_name']}\n"
                f"ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}\n"
                f"ì„±ê³¼ í‰ê°€ ìˆ˜: {total_performance}\n"
                f"í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {avg_return:.2f}%\n"
                f"í‰ê·  risk adjusted return: {avg_risk_adjusted_return:.2f}%\n"
                f"ìµœê³  ìˆ˜ìµë¥ : {max_return:.2f}%\n"
                f"ìµœì € ì†ì‹¤ë¥ : {max_loss:.2f}%\n"
            )
            send_telegram_message(settings['telegram_token'], settings['telegram_chat_id'], summary_message)
            
        except Exception as e:
            print(f"Error analyzing top performers: {e}")
            import traceback
            traceback.print_exc()
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ìš”ì•½ ë©”ì‹œì§€ë§Œ ì „ì†¡
            summary_message = ("\n=== ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===\n"
                f"ëª¨ë¸ : {settings['model_name']}\n"
                f"ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}\n"
                f"ì„±ê³¼ í‰ê°€ ìˆ˜: {total_performance}\n"
                f"í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {avg_return:.2f}%\n"
                f"í‰ê·  risk adjusted return: {avg_risk_adjusted_return:.2f}%\n"
                f"ìµœê³  ìˆ˜ìµë¥ : {max_return:.2f}%\n"
                f"ìµœì € ì†ì‹¤ë¥ : {max_loss:.2f}%\n"
            )
            send_telegram_message(settings['telegram_token'], settings['telegram_chat_id'], summary_message)

    else:
        print("ì„±ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        message = (
            "=== ê²€ì¦ ê²°ê³¼ ìš”ì•½ ===\n"
            f"ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}\n"
            f"ì„±ê³¼ í‰ê°€ ìˆ˜: {total_performance}\n"
            "ì„±ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\n"
        )
        send_telegram_message(settings['telegram_token'], settings['telegram_chat_id'], message)


def analyze_top_performers_by_date(performance_df, top_n=3):
    """ë‚ ì§œë³„ë¡œ ìƒìœ„ ì„±ê³¼ë¥¼ ë³´ì¸ ì¢…ëª©ì„ ë¶„ì„"""
    try:
        # 'prediction' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'prediction' not in performance_df.columns:
            print("Error: 'prediction' column is missing in performance_df.")
            return [], pd.DataFrame()
        
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ê¸° ì „ì— stock_nameê³¼ pattern_date ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        performance_df = performance_df.drop_duplicates(subset=['stock_name', 'pattern_date'])
        
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
        performance_df['pattern_date'] = pd.to_datetime(performance_df['pattern_date'])
        date_grouped = performance_df.groupby(performance_df['pattern_date'].dt.date)
        
        results = []
        date_summaries = []
        
        # ê° ë‚ ì§œë³„ë¡œ ì²˜ë¦¬
        for date, group in date_grouped:
            print(f"\në‚ ì§œ: {date} - Prediction ê¸°ì¤€ ìƒìœ„ {top_n}ê°œ ì¢…ëª©")
            # prediction ê¸°ì¤€ ìƒìœ„ ì¢…ëª© ì„ íƒ
            top_stocks = group.nlargest(top_n, 'prediction')
            
            # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì±„ìš°ê¸°
            if 'risk_adjusted_return' not in top_stocks.columns:
                print("Warning: 'risk_adjusted_return' column not found in top_stocks. Filling with 0.")
                top_stocks['risk_adjusted_return'] = 0  # ë˜ëŠ” ì ì ˆí•œ ê¸°ë³¸ê°’
            
            print(top_stocks[['stock_name', 'prediction', 'max_return', 'max_loss', 'estimated_profit_rate', 'risk_adjusted_return']])
            
            # ë‚ ì§œë³„ ìš”ì•½ í†µê³„
            date_summary = {
                'date': date,
                'total_patterns': len(group),
                'avg_risk_adjusted_return': group['estimated_profit_rate'].mean(),  # ìˆ˜ì •ë¨
                'avg_max_return': group['max_return'].mean(),
                'avg_max_loss': group['max_loss'].mean(),
                'top_performer': top_stocks.iloc[0]['stock_name'] if len(top_stocks) > 0 else None,
                'top_return': top_stocks.iloc[0]['estimated_profit_rate'] if len(top_stocks) > 0 else None  # ìˆ˜ì •ë¨
            }
            
            date_summaries.append(date_summary)
            results.append({'date': date, 'top_stocks': top_stocks})
        
        return results, pd.DataFrame(date_summaries)
        
    except Exception as e:
        print(f'Error analyzing top performers: {e}')
        import traceback
        traceback.print_exc()
        return [], pd.DataFrame()

def save_xgboost_predictions_to_db(predictions_df, settings):
    """XGBoost ì˜ˆì¸¡ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        db_manager = settings['buy_list_db']
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ê³  í…Œì´ë¸” í˜•ì‹ì— ë§ê²Œ ì»¬ëŸ¼ëª… ë³€ê²½
        required_columns = ['pattern_date', 'stock_name', 'estimated_profit_rate']
        
        # 'risk_adjusted_return' ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if 'risk_adjusted_return' in predictions_df.columns:
            required_columns.append('risk_adjusted_return')
        else:
            print("Warning: 'risk_adjusted_return' column not found in predictions_df.")
        
        # predictionê³¼ confidence ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        has_prediction = 'prediction' in predictions_df.columns
        has_confidence = 'confidence' in predictions_df.columns
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        dl_data = predictions_df[required_columns].copy()
        
        # predictionê³¼ confidence ì¤‘ 0ì´ ì•„ë‹Œ ê°’ì„ ì„ íƒí•˜ì—¬ confidence ì»¬ëŸ¼ì— ì €ì¥
        if has_prediction and has_confidence:
            # ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš°, 0ì´ ì•„ë‹Œ ê°’ ìš°ì„  ì‚¬ìš©
            dl_data['confidence'] = predictions_df.apply(
                lambda row: row['confidence'] if has_confidence and row['confidence'] != 0 
                            else (row['prediction'] if has_prediction else 0), 
                axis=1
            )
        elif has_prediction:
            # predictionë§Œ ìˆëŠ” ê²½ìš°
            dl_data['confidence'] = predictions_df['prediction']
        elif has_confidence:
            # confidenceë§Œ ìˆëŠ” ê²½ìš°
            dl_data['confidence'] = predictions_df['confidence']
        else:
            # ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
            print("Warning: Neither 'prediction' nor 'confidence' column found in predictions_df. Using 0.")
            dl_data['confidence'] = 0
        
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • (ì„¤ì • íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        dl_data['method'] = settings.get('model_name', 'xgboost')
        
        # ì»¬ëŸ¼ëª… ë³€ê²½
        dl_data = dl_data.rename(columns={
            'pattern_date': 'date'
        })
        
        # ê¸°ì¡´ ë°ì´í„° ì¤‘ë³µ í™•ì¸ì„ ìœ„í•œ ì½”ë“œëª…, ë‚ ì§œ, ë©”ì†Œë“œ ì¡°í•© ê°€ì ¸ì˜¤ê¸°
        existing_query = f"""
            SELECT DISTINCT date, stock_name, method FROM deep_learning
        """
        existing_data = db_manager.execute_query(existing_query)
        
        if not existing_data.empty:
            # date, stock_name, methodë¥¼ íŠœí”Œë¡œ ë¬¶ì–´ ì¤‘ë³µ í™•ì¸ìš© ì„¸íŠ¸ ìƒì„±
            existing_pairs = set()
            for _, row in existing_data.iterrows():
                # ë‚ ì§œ í˜•ì‹ í†µì¼ (ë¬¸ìì—´ ë¹„êµ ì‹œ ì˜¤ë¥˜ ë°©ì§€)
                date = pd.to_datetime(row['date']).strftime('%Y-%m-%d')
                existing_pairs.add((date, row['stock_name'], row['method']))
                
            # ì €ì¥í•  ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ ì¤‘ë³µ ì œê±°
            new_data = []
            duplicate_count = 0
            
            for idx, row in dl_data.iterrows():
                # ë‚ ì§œ í˜•ì‹ í†µì¼
                date = pd.to_datetime(row['date']).strftime('%Y-%m-%d')
                pair = (date, row['stock_name'], row['method'])
                
                if pair not in existing_pairs:
                    new_data.append(row)
                else:
                    duplicate_count += 1
            
            if duplicate_count > 0:
                print(f"Skipping {duplicate_count} duplicate entries already in the database.")
                
            if not new_data:
                print("All entries already exist in the database. Nothing to save.")
                return True
                
            # ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë§Œ ì €ì¥
            dl_data = pd.DataFrame(new_data)
            
        # ì €ì¥í•  ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥ ì§„í–‰
        if not dl_data.empty:
            result = db_manager.to_sql(dl_data, 'deep_learning')  # if_existsì™€ index íŒŒë¼ë¯¸í„° ì œê±°
            if result:
                print(f"âœ… {len(dl_data)}ê°œ {dl_data['method'].iloc[0]} ì˜ˆì¸¡ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                message = f"âœ… {len(dl_data)}ê°œ {dl_data['method'].iloc[0]} ì˜ˆì¸¡ ê²°ê³¼ë¥¼ deep_learning í…Œì´ë¸”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."
                send_telegram_message(settings['telegram_token'], settings['telegram_chat_id'], message)
            return result
        else:
            print("No new data to save after duplicate filtering.")
            return True
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False